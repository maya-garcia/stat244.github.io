[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my STAT244 Website!!",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Maya Garcia, and I am a senior at Mount Holyoke College. I am a double major in Computer Science and Statistics and I am very excited to graduate with those degrees soon. I am interested in data analysis and software programming!\n\n\n\nMount Holyoke Green Griffins!\n\n\nTo contact me, you can connect with me via Linkedin!"
  },
  {
    "objectID": "Lab9_Lasso.html",
    "href": "Lab9_Lasso.html",
    "title": "Lab 9: LASSO",
    "section": "",
    "text": "Getting Started\n\n\n\n\n\n\nDownload the .qmd file from Moodle and any needed .xlsx or .csv data files. Save these in the same folder/directory.\nOpen the Quarto file in RStudio: File &gt; Open File... &gt;. If you’re working on the MHC RStudio server, you need to upload the files first: go to the Files panel, then click Upload. Upload the .qmd file and any data files. You will need to upload each file one at a time.\nUpdate the author and date in the YAML header of this file.\nClick the Render button. If successful, you should have a new window pop up with a nice looking HTML document.\nFor this lab, you may need to still the package glmnet.\n\nAsk for help if you encounter issues on any of the steps above. Once you’ve successfully made it through these steps, you can continue."
  },
  {
    "objectID": "Lab9_Lasso.html#build-the-model-for-a-range-of-tuning-parameter-values",
    "href": "Lab9_Lasso.html#build-the-model-for-a-range-of-tuning-parameter-values",
    "title": "Lab 9: LASSO",
    "section": "Build the Model for a Range of Tuning Parameter Values",
    "text": "Build the Model for a Range of Tuning Parameter Values\n\n# STEP 1: LASSO Model Specification\nlasso_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"glmnet\") %&gt;% \n  set_args(mixture = 1, penalty = tune())\n\nError in linear_reg() %&gt;% set_mode(\"regression\") %&gt;% set_engine(\"glmnet\") %&gt;% : could not find function \"%&gt;%\"\n\n\nSTEP 1 Notes:\n\nWe use the glmnet, not lm, engine to build the LASSO.\nThe glmnet engine requires us to specify some arguments (set_args):\n\nmixture = 1 indicates LASSO. Changing this would run a different regularization algorithm.\npenalty = tune() indicates that we don’t (yet) know an appropriate \\(\\lambda\\) penalty term. We need to tune it.\n\n\nSuppose we want to build a model of response variable y using all possible predictors in a data frame sample_data.\n\n# STEP 2: Variable Recipe\nvariable_recipe &lt;- recipe(height ~ ., data = health_data) %&gt;% \n  step_dummy(all_nominal_predictors())\n\nSTEP 2 Notes:\n\ny ~ . is shorthand for “y as a function of all other variables”\nThe function step_dummy() turns all potentially categorical (sometimes called nominal) predictors into indicator variables (which are unfortunately called “dummy variables” in some circles, hence the terrible name)\n\n\n# STEP 3: Workflow Specification (Model + Recipe)\nlasso_workflow &lt;- workflow() %&gt;% \n  add_recipe(variable_recipe) %&gt;% \n  add_model(lasso_spec)\n\nSTEP 3 Notes:\n\nThe function add_recipe includes our variable_recipe created in Step 2, which is specifying what predictors/response variables we have and what data set those variables belong to\nThe function add_model includes our lasso_spec created in Step 1, which is specifying what kind of model we wish to use (in this case, regression with LASSO)\n\n\n# STEP 4: Estimate Multiple LASSO Models Using a Range of Possible Lambda Values\nset.seed(123)\nlasso_models &lt;- lasso_workflow %&gt;% \n  tune_grid(\n    grid = grid_regular(penalty(range = c(-5, 1)), levels = 50),\n    resamples = vfold_cv(health_data, v = 10),\n    metrics = metric_set(mae)\n  )\n\nSTEP 4 Notes:\n\nSince the CV process is random, we need to set.seed(___).\nWe use tune_grid() instead of fit() since we have to build multiple LASSO models, each using a different tuning parameter.\nThe function grid specifies the values of tuning parameter \\(\\lambda\\) that we want to try.\n\npenalty(range = c(___, ___)) specifies a range of \\(\\lambda\\) values we want to try, on the log10 scale.\nYou might start with c(-5, 1), which uses the range \\(\\lambda\\) from 0.00001 (\\(10^(-5)\\)) to 10 (\\(10^1\\)), and adjust from there.\nThe function levels is the number of \\(\\lambda\\) values to try in that range, thus how many LASSO models to build.\n\nThe functionsresamples and metrics indicate that we want to calculate a CV MAE (since mae is in metric_set) for each LASSO model. The number of folds in our cross-validation is given by v."
  },
  {
    "objectID": "Lab9_Lasso.html#tuning-lambda",
    "href": "Lab9_Lasso.html#tuning-lambda",
    "title": "Lab 9: LASSO",
    "section": "Tuning \\(\\lambda\\)",
    "text": "Tuning \\(\\lambda\\)\n\nPlotting CV MAE (Y-Axis) for the LASSO Model for Each \\(\\lambda\\) (X-axis)\n\n# Calculate CV MAE for each LASSO model\nlasso_models %&gt;% collect_metrics()\n\n# Plotting option 1: plot lambda on log10 scale\nautoplot(lasso_models) + scale_x_log10()\n\n# Plotting option 2: plot lambda on original scale\nautoplot(lasso_models) + \n  scale_x_continuous() +\n  xlab(expression(lambda))\n\n# Plotting option 3: CV MAE (y-axis) with error bars (+/- 1 standard error)\n# with lambda (x-axis) on original scale\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda)) + \n  geom_errorbar(data = collect_metrics(lasso_models),\n                aes(x = penalty, ymin = mean - std_err, ymax = mean + std_err), \n                alpha = 0.5)\n\n\n# Identify lambda which produced the lowest (\"best\") CV MAE\nbest_penalty &lt;- lasso_models %&gt;% \n  select_best(metric = \"mae\")\nbest_penalty\n\n# Identify the largest lambda for which the CV MAE is\n# larger but \"roughly as good\" (within one standard error of the lowest)\nparsimonious_penalty &lt;- lasso_models %&gt;% \n  select_by_one_std_err(metric = \"mae\", desc(penalty))\nparsimonious_penalty\n\n\n\nFinalizing the “Best” LASSO Model\n\n# Parameters = final lambda value (best_penalty or parsimonious_penalty)\nfinal_lasso_model &lt;- lasso_workflow %&gt;% \n  finalize_workflow(parameters = ___) %&gt;% \n  fit(data = sample_data)\n\n# Check it out\nfinal_lasso_model %&gt;% tidy()\n\n\n\nUsing Final LASSO Model to Make Predictions\n\nfinal_lasso_model %&gt;% \n  predict(new_data = SOME DATA FRAME W/ OBSERVATIONS ON EACH PREDICTOR)\n\n\n\nVisualizing Shrinkage\nThis code can help us visualize shrinkage by comparing LASSO coefficients under each \\(\\lambda\\).\n\n# Get output for each LASSO\nall_lassos &lt;- final_lasso_model %&gt;% \n  extract_fit_parsnip() %&gt;%\n  pluck(\"fit\")\n\n# Plot coefficient paths as a function of lambda\nplot(all_lassos, xvar = \"lambda\", label = TRUE, col = rainbow(20))\n\n# Codebook for which variables the numbers correspond to\nrownames(all_lassos$beta)\n\n# For example, what are variables 2 and 4?\nrownames(all_lassos$beta)[c(2,4)]"
  },
  {
    "objectID": "Lab9_Lasso.html#lasso-least-absolute-shrinkage-and-selection-operator",
    "href": "Lab9_Lasso.html#lasso-least-absolute-shrinkage-and-selection-operator",
    "title": "Lab 9: LASSO",
    "section": "LASSO: least absolute shrinkage and selection operator",
    "text": "LASSO: least absolute shrinkage and selection operator\nIdea\nPenalize a predictor for adding complexity to the model (by penalizing its coefficient). Then track whether the predictor’s contribution to the model (lowering RSS) is enough to offset this penalty.\nCriterion\nIdentify the model coefficients \\(\\hat{\\beta}_1, \\hat{\\beta}_2, ...  \\hat{\\beta}_p\\) that minimize the penalized residual sum of squares:\n\\[SSR + \\lambda \\sum_{j=1}^p \\vert \\hat{\\beta}_j\\vert = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 + \\lambda \\sum_{j=1}^p \\vert \\hat{\\beta}_j\\vert\\]\nwhere\n\nsum of squared residuals (SSR) measures the overall model prediction error\nthe penalty term measures the overall size of the model coefficients\n\\(\\lambda \\ge 0\\) (“lambda”) is a tuning parameter\n\n\nCOMMENT: Picking \\(\\lambda\\)\nWe cannot know the “best” value for \\(\\lambda\\) in advance. This varies from analysis to analysis.\nWe must try a reasonable range of possible values for \\(\\lambda\\). This also varies from analysis to analysis.\nIn general, we have to use trial-and-error to identify a range that is…\n\nwide enough that it doesn’t miss the “best” values for \\(\\lambda\\)\nnarrow enough that it focuses on reasonable values for \\(\\lambda\\)\n\nWe will explore what this means in more detail in one of the exercises."
  },
  {
    "objectID": "Lab9_Lasso.html#exercise-1",
    "href": "Lab9_Lasso.html#exercise-1",
    "title": "Lab 9: LASSO",
    "section": "EXERCISE 1",
    "text": "EXERCISE 1\nLet’s compare the CV MAEs (y-axis) for our 50 LASSO models which used 50 different \\(\\lambda\\) values (x-axis):\n\n# This is copied exactly from the R Code Notes for LASSO Section\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda))\n\nError in autoplot(lasso_models): could not find function \"autoplot\"\n\n\n\nWe told R to use a range of \\(\\lambda\\) from -5 to -0.1 on the log10 scale. Calculate this range on the non-log scale and confirm that it matches the x-axis.\n\nWhat can we observe from this plot?\n\nANSWER."
  },
  {
    "objectID": "Lab9_Lasso.html#exercise-2",
    "href": "Lab9_Lasso.html#exercise-2",
    "title": "Lab 9: LASSO",
    "section": "EXERCISE 2",
    "text": "EXERCISE 2\n\nIn the plot above, roughly which value of the \\(\\lambda\\) penalty parameter produces the smallest CV MAE?\n\n\nANSWER.\n\nCheck your approximation with code.\n\n# This is copied exactly from the R Code Notes for LASSO Section\nbest_penalty &lt;- lasso_models %&gt;% \n  select_best(metric = \"mae\")\n\nError in lasso_models %&gt;% select_best(metric = \"mae\"): could not find function \"%&gt;%\"\n\nbest_penalty\n\nError: object 'best_penalty' not found\n\n\n\nSuppose we prefer a parsimonious model.\n\nThe “parsimonious model” is the one with the fewest necessary variables while still maintaining predictive accuracy.\nIn our case, we will define it as the model with the largest possible \\(\\lambda\\) (i.e., biggest penalty for adding new variables) that still has a CV MAE within 1 standard error of the “best” model (the model whose \\(\\lambda\\) gives the lowest CV MAE).\nThe plot below adds error bars to the CV MAE estimates of prediction error (+/- one standard error). Any model with a CV MAE that falls within another model’s error bars is not significantly better or worse at prediction:\n\n# This is copied exactly from the R Code Notes for LASSO Section\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda)) + \n  geom_errorbar(data = collect_metrics(lasso_models),\n                aes(x = penalty, ymin = mean - std_err, ymax = mean + std_err), alpha = 0.5)\n\nError in autoplot(lasso_models): could not find function \"autoplot\"\n\n\nUse this to approximate the largest \\(\\lambda\\), thus the most simple LASSO model, that produces a CV MAE that’s within 1 standard error of the best model (thus is not significantly worse).\n\nANSWER.\n\nCheck your approximation with code.\n\n# This is copied exactly from the R Code Notes for LASSO Section\nparsimonious_penalty &lt;- lasso_models %&gt;% \n  select_by_one_std_err(metric = \"mae\", desc(penalty))\n\nError in lasso_models %&gt;% select_by_one_std_err(metric = \"mae\", desc(penalty)): could not find function \"%&gt;%\"\n\nparsimonious_penalty\n\nError: object 'parsimonious_penalty' not found\n\n\n\nMoving forward, we’ll use the parsimonious LASSO model. Simply report the tuning parameter \\(\\lambda\\) here.\n\n\nANSWER.\n\n\nPicking a Range to Try for \\(\\lambda\\)\nThe range of values we tried for \\(\\lambda\\) had the following nice properties. (If it didn’t, we should adjust our range (make it narrower or wider).\n\nOur range was wide enough.\n\nThe “best” and “parsimonious” \\(\\lambda\\) values were not at the edges of the range, suggesting there aren’t better \\(\\lambda\\) values outside our range.\n\nOur range was narrow enough.\n\nWe didn’t observe any loooooong flat lines in CV MAE. Thus, we narrowed in on the \\(\\lambda\\) values where the “action is happening”, i.e. where changing \\(\\lambda\\) impacts the model.\n\n\n\n\nEXERCISE 3\nModify your previous code to start with \\(\\lambda\\) in the range 10^(-5) to 10^(-0.1) and see what you observe.\n\nANSWER"
  },
  {
    "objectID": "Lab9_Lasso.html#exercise-4-finalizing-our-lasso-model",
    "href": "Lab9_Lasso.html#exercise-4-finalizing-our-lasso-model",
    "title": "Lab 9: LASSO",
    "section": "EXERCISE 4: Finalizing our LASSO model",
    "text": "EXERCISE 4: Finalizing our LASSO model\nLet’s finalize our parsimonious LASSO model:\n\nfinal_lasso &lt;- lasso_workflow %&gt;% \n  finalize_workflow(parameters = parsimonious_penalty) %&gt;% \n  fit(data = health_data)\n\nError in lasso_workflow %&gt;% finalize_workflow(parameters = parsimonious_penalty) %&gt;% : could not find function \"%&gt;%\"\n\nfinal_lasso %&gt;% \n  tidy()\n\nError in final_lasso %&gt;% tidy(): could not find function \"%&gt;%\"\n\n\n\nHow many and which predictors were kept in this model?\n\n\nANSWER.\n\n\nThrough shrinkage, the LASSO coefficients(the \\(\\hat\\beta_i\\)) lose some contextual meaning, so we typically shouldn’t interpret them. Why? THINK: What is the goal of LASSO modeling?\n\n\nANSWER."
  },
  {
    "objectID": "Lab8_VariableSubsetSelection.html",
    "href": "Lab8_VariableSubsetSelection.html",
    "title": "Variable Subset Selection",
    "section": "",
    "text": "Best Subset Selection (Notes)\n\nWith \\(p\\) predictors, there are \\(2^p\\) possible models because there are \\(2^p\\) possible subsets of the \\(p\\) predictors. (Size of the powerset) – alternatively, two choices for each predictor, either include or not include\nIn best subset selection procedure, we fit all \\(2^p\\) models and choose the one with the best value of our chosen evaluation metric (e.g., test error estimate based on CV, using some dedicated test set, etc.). Our “evaluation metric” (equivalently, “error metric”) should fairly assess test performance of model (as opposed to training set performance)\n\\(2^p\\) models? This sounds expensive.\nExample: \\(p = 4\\), 16 potential models to fit (including the zero variable model, i.e., one with just an intercept)\nDoesn’t try to transform variables or include interaction terms – would have to manually include transformed variables or interaction models in model\n\n\n\nForward Stepwise Selection\n\nIdea: add variables one at a time, choosing the “best” variable each time\n\nThis is known as a greedy algorithm: goes for temporarily/locally best choice without thinking about some kind of long-term optimum\n\nHistorical note: many past (and current) implementations to determine which variable is “best” is to choose the one whose inclusion had the lowest p-value. Problematic: discussed at the end.\n\nBetter to use CV estimates of test error instead of choices based on p-values\n\n\n\n\nGeneral Notes\n\nBackwards and forwards selection can give different results due to greedy behavior of each\nWhen several “reasonable” methods (e.g., forward/backward selection if best subset is prohibitively expensive), best practice is to try all methods, compare their results, and report all results\n\n\n\nCautions\n\nSome machine learning practitioners don’t like automated selection methods, b/c encourage us to not think about the variables b/c can just dump data into an algorithm and get a model\nNeed to think of variables in context, and carelessly using automated procedures can do real harm\nUsing p-values for deciding which variables to add/remove results gives unstable (and often “undesireable”) results when several quantitative predictors are correlated with one another (called collinearity – e.g., one predictor is a linear combination of others) – not the same as a violated independence assumption\n\nStatistical inference problems on final selected model: chosen through multiple hypothesis testing * Because we come to select a final model by trying a large number of models, multiple hypothesis testing is a big issue.\n\nWith multiple testing, the idea is that testing many hypotheses runs the risk of a result being statistically significant just by chance.\nMight have to do a Bonferroni correction or something like that (ignore this comment if you haven’t taken STAT 242)\n\nCIs are misleadingly narrow (from multiple hyp. testing)\nP-values are misleadingly small (from mult. hyp testing)\n\n\n\n\nSummary\nSubset selection methods use model quality metrics to search through different models in an automated way\n\nBest subset selection: intuitive, accurate, quickly computationally expensive\nStepwise selection (either forward or backwards): faster but not guaranteed to find “best model” due to greedy nature\nNo methods automatically consider transformations or interactions (we manually include those to exlore them)\nProblems with statistical inference\n\nShrinkage/regularization methods are a popular alternative to subset selection.\n\n\nFor Fun: Heat Map for Exploring Multicollinearity\n\n# Load packages\nlibrary(tidyverse)\n\nError in library(tidyverse): there is no package called 'tidyverse'\n\nlibrary(tidymodels)\n\nError in library(tidymodels): there is no package called 'tidymodels'\n\nlibrary(readxl)\n\nError in library(readxl): there is no package called 'readxl'\n\n# Load data\nhealth_data = read_xlsx(\"healthdata.xlsx\")\n\nError in read_xlsx(\"healthdata.xlsx\"): could not find function \"read_xlsx\"\n\nhead(health_data)\n\nError: object 'health_data' not found\n\n\n\n# Get the correlation matrix\nlibrary(reshape2)\n\nError in library(reshape2): there is no package called 'reshape2'\n\ncor_matrix &lt;- cor(health_data)\n\nError: object 'health_data' not found\n\ncor_matrix[lower.tri(cor_matrix)] &lt;- NA\n\nError: object 'cor_matrix' not found\n\ncor_matrix &lt;- cor_matrix %&gt;% \n  melt() %&gt;% \n  na.omit() %&gt;% \n  rename(correlation = value)\n\nError in cor_matrix %&gt;% melt() %&gt;% na.omit() %&gt;% rename(correlation = value): could not find function \"%&gt;%\"\n\n# Visualize the correlation for each pair of variables\nggplot(cor_matrix, aes(x = Var1, y = Var2, fill = correlation)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(\n    low = \"blue\", high = \"red\", mid = \"white\", \n    midpoint = 0, limit = c(-1,1)) +\n  labs(x = \"\", y = \"\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + \n  coord_fixed()\n\nError in ggplot(cor_matrix, aes(x = Var1, y = Var2, fill = correlation)): could not find function \"ggplot\"\n\n\n\n# STEP 1: Model Specification\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\nError in linear_reg() %&gt;% set_mode(\"regression\") %&gt;% set_engine(\"lm\"): could not find function \"%&gt;%\"\n\n# STEP 2: Model estimation\nheight_model_1 &lt;- lm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, data = health_data)\n\nError in lm_spec %&gt;% fit(height ~ age + weight + neck + chest + abdomen + : could not find function \"%&gt;%\"\n\n# Look at model\nheight_model_1 %&gt;% tidy()\n\nError in height_model_1 %&gt;% tidy(): could not find function \"%&gt;%\"\n\n\n\n\nBest Subset Selection Algorithm\n\nBuild all possible models that use any combination of the available predictors\nIdentify the best model with respect to some chosen metric (eg: CV MAE, CV MSE) and context.\n\n\n\nExercise\nSuppose we used this algorithm for our height model with all 12 possible predictors (13 variables total, one is height, leaving 12 predictors). What’s the main drawback?\n\nANSWER.\n\n\n\nBackward Stepwise Selection Algorithm\n\nBuild a model with all \\(p\\) possible predictors,\nRepeat the following until only 1 predictor remains in the model:\n\nRemove the 1 predictor that increases the MSE/MAE by the least\nBuild a model with the remaining predictors.\n\n\nYou now have \\(p\\) competing models: one with all \\(p\\) predictors, one with \\(p-1\\) predictors, …, and one with 1 predictor. In a future HW assignment, you will implement the above using MSE/MAE error metrics.\nFor this example though, for simplicity, we will identify the “best” model with respect to p-values.\nLet’s try out the first few steps!\nFirst, we would compute the 10-fold MAE of the model with all 12 predictors.\n\nset.seed(244)\nmodel_12_predictors_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n   height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n    resamples = vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae)\n  )\n\nError in lm_spec %&gt;% fit_resamples(height ~ age + weight + neck + chest + : could not find function \"%&gt;%\"\n\nmodel_12_predictors_cv %&gt;%  collect_metrics()\n\nError in model_12_predictors_cv %&gt;% collect_metrics(): could not find function \"%&gt;%\"\n\n\nThe model with all 12 predictors has a MAE of about 5.55.\nThen let’s look at which predictor to remove to identify the “best” model with 11 predictors.\n\n# Original model with 12 predictors\n# Find the \"least significant\" predictor\nlm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n      data = health_data) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4))\n\nError in lm_spec %&gt;% fit(height ~ age + weight + neck + chest + abdomen + : could not find function \"%&gt;%\"\n\n\nLooks like we should remove biceps as predictor, bc it appears to be the least statisticlly significant!\nWe would then compute the 10-fold MAE of the model with 11 predictors.\n\n#COMPUTE MAE OF MODEL WITH 11 PREDICTORS SINCE BICEPS WAS REMOVED\nset.seed(244)\nmodel_11_predictors_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n   height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle  + forearm + wrist,\n    resamples = vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae)\n  )\n\nError in lm_spec %&gt;% fit_resamples(height ~ age + weight + neck + chest + : could not find function \"%&gt;%\"\n\nmodel_11_predictors_cv %&gt;%  collect_metrics()\n\nError in model_11_predictors_cv %&gt;% collect_metrics(): could not find function \"%&gt;%\"\n\n\nWith 11 predictors, we can see that our MAE is 5.55, the full number shows a slight difference.\n\n# 11 predictors\n# UPDATE this code\nlm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + forearm + wrist,\n      data = health_data) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4))\n\nError in lm_spec %&gt;% fit(height ~ age + weight + neck + chest + abdomen + : could not find function \"%&gt;%\"\n\n\nLooks like we should remove ‘neck’ becayse it is the least statistically significant (p = 0.8880)\n\nset.seed(244)\nmodel_10_predictors_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n   height ~ age + weight  + chest + abdomen + hip + thigh + knee + ankle  + forearm + wrist,\n    resamples = vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae)\n  )\n\nError in lm_spec %&gt;% fit_resamples(height ~ age + weight + chest + abdomen + : could not find function \"%&gt;%\"\n\nmodel_10_predictors_cv %&gt;%  collect_metrics()\n\nError in model_10_predictors_cv %&gt;% collect_metrics(): could not find function \"%&gt;%\"\n\n\nNow when we removed neck and only have 10 predictors, we see that our MAE is 5.51!\n\n# 10 predictors\n# UPDATE this code by removing neck\nlm_spec %&gt;% \n  fit(height ~ age + weight  + chest + abdomen + hip + thigh + knee + ankle  + forearm + wrist,\n      data = health_data) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4))\n\nError in lm_spec %&gt;% fit(height ~ age + weight + chest + abdomen + hip + : could not find function \"%&gt;%\"\n\n\nNow from the results, we see that we should remove knee since the pvalue is 0.8497 because it is the most statistically insignificant.\n\nset.seed(244)\nmodel_9_predictors_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n   height ~ age + weight  + chest + abdomen + hip + thigh  + ankle  + forearm + wrist,\n    resamples = vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae)\n  )\n\nError in lm_spec %&gt;% fit_resamples(height ~ age + weight + chest + abdomen + : could not find function \"%&gt;%\"\n\nmodel_9_predictors_cv %&gt;%  collect_metrics()\n\nError in model_9_predictors_cv %&gt;% collect_metrics(): could not find function \"%&gt;%\"\n\n\n\n\nBackward Stepwise Selection Step-by-Step Results\nBelow is the complete model sequence along with 10-fold CV MAE for each model (using set.seed(244)).\n\n\n\npred\nCV MAE\npredictor list\n\n\n\n\n12\n5.555\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist, knee, neck, biceps\n\n\n11\n5.553\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist, knee, neck\n\n\n10\n5.512\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist, knee\n\n\n9\n5.425\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist\n\n\n8\n5.047\nweight, hip, forearm, thigh, chest, abdomen, age, ankle\n\n\n7\n5.013\nweight, hip, forearm, thigh, chest, abdomen, age\n\n\n6\n4.684\nweight, hip, forearm, thigh, chest, abdomen\n\n\n5\n4.460\nweight, hip, forearm, thigh, chest\n\n\n4\n4.386\nweight, hip, forearm, thigh\n\n\n3\n4.091\nweight, hip, forearm\n\n\n2\n3.733\nweight, hip\n\n\n1\n3.658\nweight"
  },
  {
    "objectID": "Intro-CV.html",
    "href": "Intro-CV.html",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "",
    "text": "Getting Started\n\n\n\n\n\n\nDownload the .qmd file from Moodle and any needed .xlsx or .csv data files. Save these in the same folder/directory.\nOpen the Quarto file in RStudio: File &gt; Open File... &gt;. If you’re working on the MHC RStudio server, you need to upload the files first: go to the Files panel, then click Upload. Upload the .qmd file and any data files. You will need to upload each file one at a time.\nUpdate the author and date in the YAML header of this file.\nClick the Render button. If successful, you should have a new window pop up with a nice looking HTML document.\n\nAsk for help if you encounter issues on any of the steps above. Once you’ve successfully made it through these steps, you can continue."
  },
  {
    "objectID": "Intro-CV.html#context",
    "href": "Intro-CV.html#context",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "Context",
    "text": "Context\n\nBroader subject area: supervised learning\nWe want to build a model for some output RV \\(Y\\) given various predictor RVs \\(X_1, \\ldots, X_p\\).\nTask: regression\n\\(Y\\) is quantitative (takes numerical values)\nAlgorithm: linear regression model\nWe’ll assume that the relationship between \\(Y\\) and \\(X\\) can be represented by\n\n\\[\\mathbb{E}(Y \\mid X_1, \\ldots, X_p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p  \\]"
  },
  {
    "objectID": "Intro-CV.html#review-k-fold-cross-validation",
    "href": "Intro-CV.html#review-k-fold-cross-validation",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "Review: \\(k\\)-Fold Cross Validation",
    "text": "Review: \\(k\\)-Fold Cross Validation\nWe can use k-fold cross-validation to estimate the typical error in our model predictions for new data:\n\nDivide the data into \\(k\\) folds (or groups) of approximately equal size.\n\nRepeat the following procedures for each fold \\(j = 1,2,...,k\\):\n\nRemove fold \\(j\\) from the data set.\n\nFit a model using the data in the other \\(k-1\\) folds (training).\n\nUse this model to predict the responses for the \\(n_j\\) cases in fold \\(j\\): \\(\\hat{y}_1, ..., \\hat{y}_{n_j}\\).\n\nCalculate the MAE for fold \\(j\\) (testing): \\(\\text{MAE}_j = \\frac{1}{n_j}\\sum_{i=1}^{n_j} |y_i - \\hat{y}_i|\\).\n\nCombine this information into one measure of model quality: \\[\\text{CV}_{(k)} = \\frac{1}{k} \\sum_{j=1}^k \\text{MAE}_j\\]\n\n\n\n\n\n\n\n\nVocabulary\n\n\n\n\n\n\nA tuning parameter is parameter or quantity upon which an algorithm depends whose value is selected or tuned to “optimize” the algorithm.\n\nFor \\(k\\)-fold CV, the tuning parameter is \\(k\\), where \\(2 \\le k \\le n\\), where \\(n\\) is the number of observations."
  },
  {
    "objectID": "Intro-CV.html#set-up",
    "href": "Intro-CV.html#set-up",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "Set Up",
    "text": "Set Up\n\nLoad Libraries\n\nlibrary(tidyverse)\n\nError in library(tidyverse): there is no package called 'tidyverse'\n\nlibrary(tidymodels)\n\nError in library(tidymodels): there is no package called 'tidymodels'\n\nlibrary(readxl)\n\nError in library(readxl): there is no package called 'readxl'\n\n# Load data\ndata(trees)\n\n\n\nRename Columns (Variables)\n\nRename Girth to diameter\nRename Height to height\n\n\n# Rename columns\ntrees = trees %&gt;% \n  rename(diameter = Girth, height = Height) %&gt;%\n  # Only have height and diameter be the columns \n  select(height, diameter)\n\nError in trees %&gt;% rename(diameter = Girth, height = Height) %&gt;% select(height, : could not find function \"%&gt;%\"\n\n\n\n\nVisualization (Scatter Plot)\n\n# Create a scatter plot\nggplot(trees, aes(x = diameter, y = height)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") \n\nError in ggplot(trees, aes(x = diameter, y = height)): could not find function \"ggplot\"\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\nHeight appears to be ___ with diameter\n\n\n\n\n\n# Step 1: Model specification\nlm_spec &lt;- linear_reg() %&gt;% \n  # Output Y is quantitative\n  set_mode(\"regression\") %&gt;%\n  # Want regression to be lienar\n  set_engine(\"lm\")\n\nError in linear_reg() %&gt;% set_mode(\"regression\") %&gt;% set_engine(\"lm\"): could not find function \"%&gt;%\"\n\n\n\n# Step 2: Model estimation\ntree_model &lt;- lm_spec %&gt;% fit( height ~ diameter, data = trees)\n\nError in lm_spec %&gt;% fit(height ~ diameter, data = trees): could not find function \"%&gt;%\""
  },
  {
    "objectID": "Intro-CV.html#fold-cross-validation",
    "href": "Intro-CV.html#fold-cross-validation",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "10-Fold Cross Validation",
    "text": "10-Fold Cross Validation\n\nProcedure\n\nRandomly split the data into 10 folds\nBuilt model 10 times, leaving 1 test fold out each time\nEvaluate each model on the test fold (using MAE/MSE and R-squared as error metrics)\n\n\n# For reproducibility\nset.seed(244)\n\ntree_model_cv = lm_spec %&gt;% \n# fit_resamples() function is for fitting on folds\nfit_resamples(\n  # Specify the relationship\n  height ~ diameter, \n  # vfold_cv makes CV folds randomly from\n  # trees data set\n  resamples = vfold_cv(trees, v = 10), \n  # Specify the error metrics\n  # (MAE, square root MSE, R^2)\n  metrics = metric_set(mae, rmse, rsq)\n  )\n\nError in lm_spec %&gt;% fit_resamples(height ~ diameter, resamples = vfold_cv(trees, : could not find function \"%&gt;%\"\n\n\n\ntree_model_cv %&gt;% collect_metrics()\n\nError in tree_model_cv %&gt;% collect_metrics(): could not find function \"%&gt;%\"\n\n\n\n\nSummarizing\n\n# Get fold-by-fold results\n# Get info for each test fold \ntree_model_cv %&gt;% unnest(.metrics) %&gt;% \nfilter(.metric == \"mae\")\n\nError in tree_model_cv %&gt;% unnest(.metrics) %&gt;% filter(.metric == \"mae\"): could not find function \"%&gt;%\"\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\nBased on my random folds above, the prediction error (MAE) was best for fold ___ and worst for fold ___"
  },
  {
    "objectID": "Intro-CV.html#exercise-1-in-sample-metrics",
    "href": "Intro-CV.html#exercise-1-in-sample-metrics",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 1: In-Sample Metrics",
    "text": "EXERCISE 1: In-Sample Metrics\nUse the health_data data to build two separate models of height:\n\n# STEP 2: model estimation\nmodel_1 &lt;- lm_spec %&gt;% \n  fit(height ~ hip + weight + thigh + knee + ankle, data = health_data)\n\nError in lm_spec %&gt;% fit(height ~ hip + weight + thigh + knee + ankle, : could not find function \"%&gt;%\"\n\nmodel_2 &lt;- lm_spec %&gt;% \n  fit(height ~ chest * age * weight  + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, data = health_data)\n\nError in lm_spec %&gt;% fit(height ~ chest * age * weight + abdomen + hip + : could not find function \"%&gt;%\"\n\n\nCalculate the in-sample R-squared for both models:\n\n# IN-SAMPLE R^2 for model_1 = ???\nmodel_1 %&gt;% glance()\n\nError in model_1 %&gt;% glance(): could not find function \"%&gt;%\"\n\n# IN-SAMPLE R^2 for model_2 = ???\nmodel_2 %&gt;% glance()\n\nError in model_2 %&gt;% glance(): could not find function \"%&gt;%\"\n\n\n\nANSWER. The R2 value for the first model is about 0.366, and for the second model, it’s 0.526.\n\nCalculate the in-sample MAE for both models:\n\n# IN-SAMPLE MAE for model_1 = ???\nmodel_1 %&gt;% \n  augment(new_data = health_data) %&gt;% \n  mae(truth = height, estimate = .pred)\n\nError in model_1 %&gt;% augment(new_data = health_data) %&gt;% mae(truth = height, : could not find function \"%&gt;%\"\n\n# IN-SAMPLE MAE for model_2 = ???\nmodel_2 %&gt;% \n  augment(new_data = health_data) %&gt;% \n  mae(truth = height, estimate = .pred)\n\nError in model_2 %&gt;% augment(new_data = health_data) %&gt;% mae(truth = height, : could not find function \"%&gt;%\"\n\n\n\nANSWER. Based on the in-sample MAE(i.e. the MAE of the same data used to build/train the model), it appears that model 2 (whose MAE is about 3.666) is better than model 1 (whose MAE is about 3.48)"
  },
  {
    "objectID": "Intro-CV.html#exercise-2-in-sample-model-comparison",
    "href": "Intro-CV.html#exercise-2-in-sample-model-comparison",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 2: In-Sample Model Comparison",
    "text": "EXERCISE 2: In-Sample Model Comparison\nWhich model seems “better” by the in-sample metrics you calculated above? Any concerns about either of these models?\n\nAnswer above! The concern is that we are using the same data that we built the modelw ith to evaluate the model’s error/performace, which means that the “better looking” model might be overfit (might be overly specific to the data used to build it)"
  },
  {
    "objectID": "Intro-CV.html#exercise-3-10-fold-cv",
    "href": "Intro-CV.html#exercise-3-10-fold-cv",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 3: 10-Fold CV",
    "text": "EXERCISE 3: 10-Fold CV\nComplete the code to run 10-fold cross-validation for our two models.\nmodel_1: height ~ hip + weight + thigh + knee + ankle\nmodel_2: height ~ chest * age * weight  + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist\n\n# 10-fold cross-validation for model_1\nset.seed(244)\nmodel_1_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    height ~ hip + weight + thigh + knee + ankle,\n    resamples= vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae, rsq)\n  )\n\nError in lm_spec %&gt;% fit_resamples(height ~ hip + weight + thigh + knee + : could not find function \"%&gt;%\"\n\n\n\n# 10-fold cross-validation for model_2\nset.seed(253)\nmodel_2_cv &lt;-lm_spec %&gt;% \n  fit_resamples(\n    height ~ chest * age * weight  + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n    resamples= vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae, rsq)\n  )\n\nError in lm_spec %&gt;% fit_resamples(height ~ chest * age * weight + abdomen + : could not find function \"%&gt;%\""
  },
  {
    "objectID": "Intro-CV.html#exercise-4-calculating-the-cv-mae",
    "href": "Intro-CV.html#exercise-4-calculating-the-cv-mae",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 4: Calculating the CV MAE",
    "text": "EXERCISE 4: Calculating the CV MAE\n\nUse collect_metrics() to obtain the cross-validated MAE and \\(R^2\\) for both models.\n\n\nmodel_1_cv %&gt;% \n  collect_metrics()\n\nError in model_1_cv %&gt;% collect_metrics(): could not find function \"%&gt;%\"\n\nmodel_2_cv %&gt;% \n  collect_metrics()\n\nError in model_2_cv %&gt;% collect_metrics(): could not find function \"%&gt;%\"\n\n\n\nInterpret the cross-validated MAE and \\(R^2\\) for model_1.\n\n\nANSWER. We expect our first model to produce predictions of height that are roughly off by 4.13 (the observed MAE) on average. For the first model, we expect i to explain roughly 0.28 (28%) of the variablity (based on the R2 value) in the observed heights of patients in the data set."
  },
  {
    "objectID": "Intro-CV.html#exercise-5-fold-by-fold-results",
    "href": "Intro-CV.html#exercise-5-fold-by-fold-results",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 5: Fold-By-Fold Results",
    "text": "EXERCISE 5: Fold-By-Fold Results\nThe command collect_metrics() gave the final CV MAE, or the average MAE across all 10 test folds. The command unnest(.metrics) provides the MAE from each test fold.\n\nObtain the fold-by-fold results for the model_1 cross-validation procedure using unnest(.metrics).\n\n\nmodel_1_cv %&gt;% \n  unnest(.metrics) %&gt;%  \n  filter (.metric == \"mae\")\n\nError in model_1_cv %&gt;% unnest(.metrics) %&gt;% filter(.metric == \"mae\"): could not find function \"%&gt;%\"\n\n\n\nWhich fold had the worst average prediction error and what was it?\n\n\nYOUR ANSWER HERE. For me, fold 5 had the worse (highest) MAE (which was 10.9)\n\n\nRecall that collect_metrics() reported a final CV MAE of ___ for model_1. Confirm this calculation by wrangling the fold-by-fold results from part a.\n\n\nmodel_1_cv %&gt;% \n  unnest(.metrics) %&gt;%  \n  filter (.metric == \"mae\")%&gt;%  \n  summarize(mean(.estimate))\n\nError in model_1_cv %&gt;% unnest(.metrics) %&gt;% filter(.metric == \"mae\") %&gt;% : could not find function \"%&gt;%\""
  },
  {
    "objectID": "Intro-CV.html#exercise-6-comparing-models",
    "href": "Intro-CV.html#exercise-6-comparing-models",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 6: Comparing Models",
    "text": "EXERCISE 6: Comparing Models\nFill in the table below to summarize the in-sample and 10-fold CV MAE for both models.\n\n\n\nModel\nIN-SAMPLE MAE\n10-fold CV MAE\n\n\n\n\nmodel_1\n3.28\n4.13\n\n\nmodel_2\n3.37\n6.28\n\n\n\n\nBased on the in-sample MAE alone, which model appears better?\n\n\nYOUR ANSWER HERE\n\n\nBased on the CV MAE alone, which model appears better?\n\n\nYOUR ANSWER HERE\n\n\nBased on all of these results, which model would you pick?\n\n\nYOUR ANSWER HERE\n\n\nDo the in-sample and CV MAE suggest that model_1 is overfit to our health_data sample data? What about model_2?\n\n\nIt looks like the MAE is roughly similar for when it’s measued in sample (3.48) versus when it is tested on “new” data (each test fold held out). However, model 2 seems overfit because its predictions for new patient data (giving on MAE of 6.28) are much worse than its predictions for patients in our data smple (MAE of 3.37)"
  },
  {
    "objectID": "Intro-CV.html#exercise-7-loocv",
    "href": "Intro-CV.html#exercise-7-loocv",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 7: LOOCV",
    "text": "EXERCISE 7: LOOCV\n\nReconsider model_1. Instead of estimating its prediction accuracy using the 10-fold CV MAE, use the LOOCV MAE. THINK: How many people are in our health_data sample?\n\n\nmodel_1_loocv = lm_spec %&gt;%\n  fit_resamples(\n    height ~ hip + weight + thigh + knee + ankle,\n    resamples = vfold_cv(health_data, v = nrow(health_data)),\n    metrics = metric_set(mae)\n  )\n\nError in lm_spec %&gt;% fit_resamples(height ~ hip + weight + thigh + knee + : could not find function \"%&gt;%\"\n\n\n\nHow does the LOOCV MAE compare to the 10-fold CV MAE of ___? NOTE: These are just two different approaches to estimating the same thing: the typical prediction error when applying our model to new data. Thus we should expect them to be similar.\n\n\nANSWER.\n\n\nExplain why we technically don’t need to set.seed() for the LOOCV algorithm.\n\n\nANSWER."
  },
  {
    "objectID": "HW8.html",
    "href": "HW8.html",
    "title": "Homework 8",
    "section": "",
    "text": "This assignment is due at 11:59 PM on Thursday, May 1st.\n\n\n\nAll problems will be graded for correctness. In grading these problems, an emphasis will be placed on full explanations of your thought process. You don’t need to write more than a few sentences for any given problem, but you should write complete sentences! Understanding and explaining the reasons behind what you are doing is at least as important as solving the problems correctly.\n\n\n\nYou are allowed to work with others on this assignment, but you must complete and submit your own write up. You should not copy large blocks of code or written text from another student.\n\n\n\nAll sources you refer to must be cited at the end of the problem set.\n\n\n\n\nFor each of these statements, state whether they are true or false, and why:\n\n\nThis is false! The significance level of a statistical test is equal to the probability of rejecting the null hypothesis when it is true, not the probability that it is true.\n\n\n\nFalse! If we decrease the significance level of a test , it makes it more difficult to reject the null hypothesis so the probability of rejecting the null hypothesis and it being incorrect will decrease. This means that our power will decrease as well.\n\n\n\nFalse. Both errors are significant but type 1 is worse as it is a false positive and will alter our findings to our conclusion.\n\n\n\nFalse! The power of a test is determined by the probability of correctly rejecting the null hypothesis when needed , when the null hypothesis is false.\n\n\n\n\nLet \\(X\\) have one of the following distributions:\nWe want to conduct the test:\n\\(H_0\\): the first distribution is correct \\(H_A\\): the second distribution is correct\n\n\n\nh0 = c(.2,.3,.3,.2)\nha = c(.1,.4,.1,.4)\n\nlambda = h0/ha\nlambda\n\n[1] 2.00 0.75 3.00 0.50\n\n\nThe likelihood ratio is\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that x4 has the lowest likelihood ratio and x3 has the highest.\n\n\n\n\ndata.frame(x = c(\"x1\", \"x2\", \"x3\",\"x4\"), lambda, h0)\n\n   x lambda  h0\n1 x1   2.00 0.2\n2 x2   0.75 0.3\n3 x3   3.00 0.3\n4 x4   0.50 0.2\n\n\nWe can see the the probability distribution of \\(\\Lambda\\) under the null hypothesis for each x is:\nx1 = 0.2\nx2 = 0.3\nx3 = 0.3\nx4 = 0.2\n\n\n\nThe possible p-values for the likelihood ratio test are :\n\nh0x4 =.2\nh0x2=.3\nh0x1=0.2\nh0x3= .3\n\n# starting from least lambda \npvalx4 = h0x4\npvalx2 = h0x4 +h0x2\npvalx1 = h0x4 +h0x2 +h0x1\npvalx3 = h0x4 +h0x2 +h0x1 +h0x3\npvalx4\n\n[1] 0.2\n\npvalx2\n\n[1] 0.5\n\npvalx1\n\n[1] 0.7\n\npvalx3\n\n[1] 1\n\n\n\n\n\n\nLet \\(X_1, X_2, \\ldots, X_n\\) be a random sample from \\(N\\left(0, \\sigma^2\\right)\\). Consider testing \\(H_0: \\sigma=\\sigma_0\\) versus \\(H_A: \\sigma=\\sigma_1\\), where \\(\\sigma_1&gt;\\sigma_0\\). The values \\(\\sigma_0\\) and \\(\\sigma_1\\) are fixed.\n\n\n\\[\nf(x_i \\mid \\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{x_i^2}{2\\sigma^2}\\right)\n\\]\nLikelihood :\n\\[\nL(\\sigma) = \\left( \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\right)^n \\exp\\left( -\\frac{\\sum x_i^2}{2\\sigma^2} \\right)\n\\]\nLikelihood ratio:\n\\[\n\\Lambda = \\frac{\n\\left( \\frac{1}{\\sqrt{2\\pi \\sigma_0^2}} \\right)^n \\cdot \\exp\\left( -\\frac{\\sum x_i^2}{2\\sigma_0^2} \\right)\n}{\n\\left( \\frac{1}{\\sqrt{2\\pi \\sigma_1^2}} \\right)^n \\cdot \\exp\\left( -\\frac{\\sum x_i^2}{2\\sigma_1^2} \\right)\n}\n\\]\nsimplifying:\n\\[\n\\Lambda = \\left( \\frac{\\sigma_1}{\\sigma_0} \\right)^n \\cdot \\exp\\left\\{ \\left(\\sum x_i^2( \\frac{1}{2\\sigma_1^2} - \\frac{1}{2\\sigma_0^2} \\right) \\right\\}\n\\]\n\n\n\nWe know that our likelihood ratio depends on the summation of x^2, formula above, this means that a large value of the summation means the test statistic decreases leading to more evidence to reject the null hypothesis.We are able to reject H0 when the summation is large. Now the pvalue is the probability of getting a test statistics assuming H0 is true, so based on our experiment, the pvalye is the chance that our summation is greater than the value we saw in data thus: \\(P(\\sum_{i=1}^n X_i^2 &gt; \\sum_{i=1}^n x_i^2 \\mid H_0 \\text{ is true})\\)\n\n\n\nWe know it follows a normal distribution so let us standardize. Our Z value is $\\frac{X_i}{\\sigma_0}$, we used \\(\\sigma_0\\) because we are assuming H0 is true. We need to square it and sum like our formula so then: \\(\\sum_{i=1}^n Z_i^2 \\frac{1}{\\sigma^2_0}=  \\sum_{i=1}^n X_i^2\\) and based on our chapter 6 notes, we can see that it follows a chi square distribution with n degrees of freedom. \\(\\frac{1}{\\sigma^2_0} \\sum_{i=1}^n X_i^2\\) ~ \\(X^2_n\\)\n\n\n\nWe need to find a \\(c\\) value where our probability of\n\\[\n\\sum_{i=1}^n X_i^2 &gt; c \\mid H_0 = \\alpha\n\\]\n\\[\n\\frac{1}{\\sigma_0^2} \\sum_{i=1}^n X_i^2 &gt; \\frac{c}{\\sigma_0^2}\n\\]\n\\[\n\\chi^2_n &gt; \\frac{c}{\\sigma_0^2}\n\\]\n\\[\n\\frac{c}{\\sigma_0^2} = \\chi^2_{1 - \\alpha, n}\n\\]\n\\[\nc = \\chi^2_{1 - \\alpha, n} \\cdot \\sigma_0^2\n\\]\n\n\n\nYes, this test is uniformly most powerful for testing our null hypothesis versus our alternative because we tested whether the standard deviation is bigger than a certain value and we based it on the statistics of the sum of squares.With normal distribution, we know that when sigma increases , our sum increasea as well and then we can use our test to correctly reject our null."
  },
  {
    "objectID": "HW8.html#problem-1",
    "href": "HW8.html#problem-1",
    "title": "Homework 8",
    "section": "",
    "text": "For each of these statements, state whether they are true or false, and why:\n\n\nThis is false! The significance level of a statistical test is equal to the probability of rejecting the null hypothesis when it is true, not the probability that it is true.\n\n\n\nFalse! If we decrease the significance level of a test , it makes it more difficult to reject the null hypothesis so the probability of rejecting the null hypothesis and it being incorrect will decrease. This means that our power will decrease as well.\n\n\n\nFalse. Both errors are significant but type 1 is worse as it is a false positive and will alter our findings to our conclusion.\n\n\n\nFalse! The power of a test is determined by the probability of correctly rejecting the null hypothesis when needed , when the null hypothesis is false."
  },
  {
    "objectID": "HW8.html#problem-2",
    "href": "HW8.html#problem-2",
    "title": "Homework 8",
    "section": "",
    "text": "Let \\(X\\) have one of the following distributions:\nWe want to conduct the test:\n\\(H_0\\): the first distribution is correct \\(H_A\\): the second distribution is correct\n\n\n\nh0 = c(.2,.3,.3,.2)\nha = c(.1,.4,.1,.4)\n\nlambda = h0/ha\nlambda\n\n[1] 2.00 0.75 3.00 0.50\n\n\nThe likelihood ratio is\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that x4 has the lowest likelihood ratio and x3 has the highest.\n\n\n\n\ndata.frame(x = c(\"x1\", \"x2\", \"x3\",\"x4\"), lambda, h0)\n\n   x lambda  h0\n1 x1   2.00 0.2\n2 x2   0.75 0.3\n3 x3   3.00 0.3\n4 x4   0.50 0.2\n\n\nWe can see the the probability distribution of \\(\\Lambda\\) under the null hypothesis for each x is:\nx1 = 0.2\nx2 = 0.3\nx3 = 0.3\nx4 = 0.2\n\n\n\nThe possible p-values for the likelihood ratio test are :\n\nh0x4 =.2\nh0x2=.3\nh0x1=0.2\nh0x3= .3\n\n# starting from least lambda \npvalx4 = h0x4\npvalx2 = h0x4 +h0x2\npvalx1 = h0x4 +h0x2 +h0x1\npvalx3 = h0x4 +h0x2 +h0x1 +h0x3\npvalx4\n\n[1] 0.2\n\npvalx2\n\n[1] 0.5\n\npvalx1\n\n[1] 0.7\n\npvalx3\n\n[1] 1"
  },
  {
    "objectID": "HW8.html#problem-3",
    "href": "HW8.html#problem-3",
    "title": "Homework 8",
    "section": "",
    "text": "Let \\(X_1, X_2, \\ldots, X_n\\) be a random sample from \\(N\\left(0, \\sigma^2\\right)\\). Consider testing \\(H_0: \\sigma=\\sigma_0\\) versus \\(H_A: \\sigma=\\sigma_1\\), where \\(\\sigma_1&gt;\\sigma_0\\). The values \\(\\sigma_0\\) and \\(\\sigma_1\\) are fixed.\n\n\n\\[\nf(x_i \\mid \\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{x_i^2}{2\\sigma^2}\\right)\n\\]\nLikelihood :\n\\[\nL(\\sigma) = \\left( \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\right)^n \\exp\\left( -\\frac{\\sum x_i^2}{2\\sigma^2} \\right)\n\\]\nLikelihood ratio:\n\\[\n\\Lambda = \\frac{\n\\left( \\frac{1}{\\sqrt{2\\pi \\sigma_0^2}} \\right)^n \\cdot \\exp\\left( -\\frac{\\sum x_i^2}{2\\sigma_0^2} \\right)\n}{\n\\left( \\frac{1}{\\sqrt{2\\pi \\sigma_1^2}} \\right)^n \\cdot \\exp\\left( -\\frac{\\sum x_i^2}{2\\sigma_1^2} \\right)\n}\n\\]\nsimplifying:\n\\[\n\\Lambda = \\left( \\frac{\\sigma_1}{\\sigma_0} \\right)^n \\cdot \\exp\\left\\{ \\left(\\sum x_i^2( \\frac{1}{2\\sigma_1^2} - \\frac{1}{2\\sigma_0^2} \\right) \\right\\}\n\\]\n\n\n\nWe know that our likelihood ratio depends on the summation of x^2, formula above, this means that a large value of the summation means the test statistic decreases leading to more evidence to reject the null hypothesis.We are able to reject H0 when the summation is large. Now the pvalue is the probability of getting a test statistics assuming H0 is true, so based on our experiment, the pvalye is the chance that our summation is greater than the value we saw in data thus: \\(P(\\sum_{i=1}^n X_i^2 &gt; \\sum_{i=1}^n x_i^2 \\mid H_0 \\text{ is true})\\)\n\n\n\nWe know it follows a normal distribution so let us standardize. Our Z value is $\\frac{X_i}{\\sigma_0}$, we used \\(\\sigma_0\\) because we are assuming H0 is true. We need to square it and sum like our formula so then: \\(\\sum_{i=1}^n Z_i^2 \\frac{1}{\\sigma^2_0}=  \\sum_{i=1}^n X_i^2\\) and based on our chapter 6 notes, we can see that it follows a chi square distribution with n degrees of freedom. \\(\\frac{1}{\\sigma^2_0} \\sum_{i=1}^n X_i^2\\) ~ \\(X^2_n\\)\n\n\n\nWe need to find a \\(c\\) value where our probability of\n\\[\n\\sum_{i=1}^n X_i^2 &gt; c \\mid H_0 = \\alpha\n\\]\n\\[\n\\frac{1}{\\sigma_0^2} \\sum_{i=1}^n X_i^2 &gt; \\frac{c}{\\sigma_0^2}\n\\]\n\\[\n\\chi^2_n &gt; \\frac{c}{\\sigma_0^2}\n\\]\n\\[\n\\frac{c}{\\sigma_0^2} = \\chi^2_{1 - \\alpha, n}\n\\]\n\\[\nc = \\chi^2_{1 - \\alpha, n} \\cdot \\sigma_0^2\n\\]\n\n\n\nYes, this test is uniformly most powerful for testing our null hypothesis versus our alternative because we tested whether the standard deviation is bigger than a certain value and we based it on the statistics of the sum of squares.With normal distribution, we know that when sigma increases , our sum increasea as well and then we can use our test to correctly reject our null."
  },
  {
    "objectID": "FP CP3.html",
    "href": "FP CP3.html",
    "title": "Final Project CP3",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n\n\n✔ broom        1.0.8     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tibble       3.2.1\n✔ infer        1.0.8     ✔ tidyr        1.3.1\n✔ modeldata    1.4.0     ✔ tune         1.3.0\n✔ parsnip      1.3.1     ✔ workflows    1.2.0\n✔ purrr        1.0.4     ✔ workflowsets 1.1.0\n✔ recipes      1.3.0     ✔ yardstick    1.3.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n\nlibrary(readxl)\n\nhsb2=read.csv(\"hsb2.csv\")"
  },
  {
    "objectID": "FP CP3.html#variable-subset-selection",
    "href": "FP CP3.html#variable-subset-selection",
    "title": "Final Project CP3",
    "section": "Variable Subset Selection",
    "text": "Variable Subset Selection\n\nIdentify at least p = 3 predictors for modeling the expected response E(Y) of one of your variables Y.\n\nWe are starting with 4 predictors which are read (standardized reading score), ses (socioeconomic status), schtyp (school type), and prog (program type). Our p =4! Our response variable is write (standardized writing score).\n\nState which error metric you are using (i.e., CV MAE or CV MSE and state the chosen k-value).\n\n\nWe will use Cross Validation MAE with k = 5.\n\nBest subset selection: Only implement this if you have under 100 rows/observations in your data set. Otherwise, skip this! It will be too computationally expensive!\n\nBuild all 2p possible models that use any combination of the available predictors. Which model do you pick?\n\nSkipping since our data has 200 observations!\n\nBackward subset selection: Implement backward subset selection by starting with at least p = 3 predictors.\n\nWe start with p = 4 predictors and we perform backward subset selection. We can see that the one with the lowest MAE is with all the predictors.\n\n\n\nlm_spec = linear_reg()%&gt;% set_engine(\"lm\") %&gt;% set_mode(\"regression\")\n\nmodel_4predictors = lm_spec %&gt;% fit_resamples(write~ read + ses + schtyp + prog, resamples = vfold_cv(hsb2, v = 5), metrics = metric_set(mae) )\nmodel_4predictors %&gt;% collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    6.14     5   0.285 Preprocessor1_Model1\n\nmodel_3predictors = lm_spec %&gt;% fit_resamples(write~ read + ses + schtyp , resamples = vfold_cv(hsb2, v = 5), metrics = metric_set(mae) )\nmodel_3predictors %&gt;% collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    6.34     5   0.279 Preprocessor1_Model1\n\nmodel_2predictors = lm_spec %&gt;% fit_resamples(write~ read + ses , resamples = vfold_cv(hsb2, v = 5), metrics = metric_set(mae) )\nmodel_2predictors %&gt;% collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    6.35     5   0.390 Preprocessor1_Model1\n\nmodel_1predictors = lm_spec %&gt;% fit_resamples(write~ read , resamples = vfold_cv(hsb2, v = 5), metrics = metric_set(mae) )\nmodel_1predictors %&gt;% collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    6.29     5   0.367 Preprocessor1_Model1\n\n\n\nWe have to pick a value for our tuning parameter (the number of predictors). Plot your cross validated error as a function of the number of predictors, where each model has predictors chosen based on your backward subset selection.\n\nAs we can see again, our lowest MAE is from our model with all four predictors\n\n\n\nresults = bind_rows( model_1predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 1),model_2predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 2),model_3predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 3),model_4predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 4))\n\nggplot(results , aes(x = predictors, y = mean)) + geom_point(color = \"lightblue\") + geom_line(color = \"lightblue\") + theme_classic()\n\n\n\n\n\n\n\n\n\nBased on your plot, which model do you pick?\n\nWe pick our model with all four predictors.\n\nWhy is backward subset selection a greedy algorithm? Answer in 1 - 3 sentences.\n\nBackwards subset selection is a greedy algorithm because it removes one predictor each time and uses computer runtime for it. This also means that it removes previous predictors, and often misses the most optimal predictor combination."
  },
  {
    "objectID": "FP CP3.html#source",
    "href": "FP CP3.html#source",
    "title": "Final Project CP3",
    "section": "Source:",
    "text": "Source:\nNational Center for Education Statistics - Statistical Consulting. (n.d.). High School & Beyond (HS&B) - overview. High School & Beyond (HS&B) - Overview. https://nces.ed.gov/surveys/hsb/\nExploring Data with Graphic; R Learning Modules. UCLA: Statistical Consulting Group. from https://stats.oarc.ucla.edu/r/modules/exploring-data-with-graphics/ (accessed April 28, 2025)."
  },
  {
    "objectID": "FP CP2_AHO.html",
    "href": "FP CP2_AHO.html",
    "title": "Final Project CP2",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2"
  },
  {
    "objectID": "FP CP2_AHO.html#part-1-data-context",
    "href": "FP CP2_AHO.html#part-1-data-context",
    "title": "Final Project CP2",
    "section": "Part 1: Data Context",
    "text": "Part 1: Data Context\n\nWhat variables in your data set are you interested in? Are they quantitative or categorical?\nIf quantitative, provide the units. If categorical, list or describe the categories.\n\n\nWe are interested in the following variables.\n\n‘write’: Standardized writing score from various tests; measured in standard errors.\n‘read’: standardized reading scores from various tests; measured in standard errors.\n‘ses’: Socioeconomic status of student’s family. Categories: low, middle, high\n‘schtyp’: Type of school attending. Categories: public, private\n‘prog’: Type of program enrolled in. Categories: general, academic, vocational\n\n\n\nWhat does one observational unit (row) represent in your data set?\n\n\nOne observational unit in our data set represents one High School student who participated in the National Center of Education Statistics survey titled “High School and Beyond”.\n\n\nHow was the sample obtained? If you can find what the sampling method was, include that information. Also include what year the data was collected\n\n\nThe data was obtained through surveys of 1,100 10th-12th graders in 1980 conducted by the National Center for Education Statistics. The data we are using in our project is a random sample of size 200 taken from the data in 2016 by UCLA Institute for Digital Research & Education Statistical Consulting. There are no specifics of how this sample was taken.\n\n\nCould the data collectors have an ulterior motive for collecting the data (e.g., solicitation of private information)? Could the data collectors have an ulterior motive for collecting a biased sample, or otherwise misrepresenting the population in any way (e.g., trying to reinforce a predetermined narrative)?\n\n\nThere is no clear ulterior motive for collecting this data. Although standardized test scores may influence government funding for particular districts and could pose as an ulterior motive for collecting the data, this sample of 200 students is too small to have an influence on any particular school district. There is also no data collected in the sample on where students attend school.\n\n\nDo you think the source of your data is reliable? Do you trust how the data was collected?\n\nYes, we believe the source of our data is reliable. It was collected by the “federal statistical agency responsible for collecting, analyzing, and reporting data on the condition of U.S. education.” Our data was sampled by UCLA, which is a reliable academic source.\nThe data was collected via survey, thus ome survey bias may be present in the data. It should be noted that there is no clear indication of such bias."
  },
  {
    "objectID": "FP CP2_AHO.html#part-2-data-cleaning",
    "href": "FP CP2_AHO.html#part-2-data-cleaning",
    "title": "Final Project CP2",
    "section": "Part 2: Data Cleaning",
    "text": "Part 2: Data Cleaning\nLoading our data\n\nhsb2=read.csv(\"hsb2.csv\")\nhead(hsb2)\n\n   id gender  race    ses schtyp       prog read write math science socst\n1  70   male white    low public    general   57    52   41      47    57\n2 121 female white middle public vocational   68    59   53      63    61\n3  86   male white   high public    general   44    33   54      58    31\n4 141   male white   high public vocational   63    44   47      53    56\n5 172   male white middle public   academic   47    52   57      53    61\n6 113   male white middle public   academic   44    52   51      63    61\n\n\n\nDo the variables you listed in (1) have names that are easy to use in code? (Examples of names that might not be easy to work with are those with spaces, those that are very long, etc.). If not, use mutate() and select() to rename your variables (or the rename() function).\n\nYes! All of our values are simple and short enough that we do not need to change any of the names!\n\n\ncolnames(hsb2)\n\n [1] \"id\"      \"gender\"  \"race\"    \"ses\"     \"schtyp\"  \"prog\"    \"read\"   \n [8] \"write\"   \"math\"    \"science\" \"socst\"  \n\n# No need to improve variable nams as our variables names in hsb2 are simple enough\n\nDo your quantitative variables have the correct types (e.g., double for decimals, integer for whole numbers, etc.). If not, set them to be the correct type. You can check the type of a variable using the class() function.\n\n#Checking our quantitative variable types\nsapply(hsb2[,c(\"read\",\"write\")], class)\n\n     read     write \n\"integer\" \"integer\" \n\n\n\nAll our quantitative variables have type double, so we are all good!\n\nDo your categorical variables have the type factor? If not, give them this type to tell R that they are categorical.\n\n#Checking our cateforial variable types\nsapply(hsb2[,c (\"ses\",\"schtyp\",\"prog\")], class)\n\n        ses      schtyp        prog \n\"character\" \"character\" \"character\" \n\n#Changing them to type factor\nhsb2$ses = as.factor(hsb2$ses)\nhsb2$schtyp = as.factor(hsb2$schtyp)\nhsb2$prog = as.factor(hsb2$prog )\n\n\nWhen we checked our variable type for our categorical variables, we saw they were as type ‘character’. We then changed them to type factor!\n\nAre there any variables you need to create? (For example, you might want a variable that combines existing categories, a categorical version of a quantitative variable, etc.) \n\nNo there is no need to create new variables with the research we want from this data set!\n\nAre there any missing values for your variables listed in (9)? If so, filter them out. Note: Some data sets use empty strings “” to denote missing values, so check for these as well.\n\nnrow(hsb2) #checking our starting number of rows\n\n[1] 200\n\n# Removing missing data points\nhsb2= hsb2 %&gt;% filter(!is.na(read), !is.na(write),!is.na(ses),!is.na(schtyp),!is.na(prog))\nnrow(hsb2)\n\n[1] 200\n\nhsb2 %&gt;% ggplot(aes(x = read)) +geom_bar(fill = \"lightblue\") +theme_classic()  +\nlabs(x = 'Standardized Reading Test Scores', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = write)) +geom_bar(fill = \"lightblue\") +theme_classic()  +\nlabs(x = 'Standardized Writing Test Scores', y = 'Count')\n\n\n\n\n\n\n\n# Checking for any empty strings\nhsb2 %&gt;% ggplot(aes(x = ses)) +geom_bar(fill = \"lightblue\") +theme_classic()  +\nlabs(x = 'Socioeconomic status', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = schtyp)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'School Type', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = prog)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'Program Type', y = 'Count')\n\n\n\n\n\n\n\n\n\nWe started off with 200 rows and then checked for any NA values, and we saw there was none to remove! We also used plots to check for any empty string values and we did not find any!\n\nHow many observational units (rows) do you have after the previous step?\n\nWe still have 200 rows after our previous step.\n\nDoes the source of data say that certain data points are missing? (For example, in NHANES we are told that the variable Height is measured only for participants aged 2 years or older, meaning Height will have NA values in rows corresponding to participants under 2.) If not available in the data description, why do you think some data points are missing? Is the mechanism for missingness completely random, or could there be something systematic which leads to greater rates of missingness? It’s okay if you’re not sure how to answer this question — just do your best to think about it.\n\nYes, our data randomly sampled 200 data points from High School and Beyond Survey from 2016. This data was collected of high school seniors so this does not include other grade levels. We were not able to find how the data was randomly sampled as well. From our previous step, we also did not find anything rows to remove.\n\n\n\n#Comparing multiple variables\nggpairs(hsb2 %&gt;% select(write,read,ses,schtyp,prog))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "FP CP2_AHO.html#part-3-exploratory-data-analysis",
    "href": "FP CP2_AHO.html#part-3-exploratory-data-analysis",
    "title": "Final Project CP2",
    "section": "Part 3: Exploratory Data Analysis",
    "text": "Part 3: Exploratory Data Analysis\n\nProvide any numerical summaries that are relevant to your research question. These are highly dependent on your question, but some ideas are:\n\n\nFor one quantitative variable, some relevant numerical summaries involve:\n\nOne measure of center (e.g., mean, median);\nOne measure of spread (e.g., standard deviation, variance, IQR, a minimum/maximum);\nAn overall count (how many observations do you have?)\n\nFor one categorical variable, a relevant numerical summary might be:\n\nthe # of observations in each category (i.e., use count());\nthe proportion (or percentage) of observations in each category.\n\n\n\nOur research question we are trying to find is how are writing scores affected by multiple variables of the highschool senior. The variables we are going to look at is reading scores, socioeconomic status, school type, and program type.\nLets first review our quantitative variables. For our standardized writing scores, our mean score is around 52.77 with range from 31 to 67. Our standard deviation is rounded to 9.5. For our standardized reading scores, our mean is 52.23, with range from 28 to 76. Our standard deviation for these scores is rounded to 10.3. We have 200 observations.\n\n\nsummary(hsb2$write)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  31.00   45.75   54.00   52.77   60.00   67.00 \n\nsd(hsb2$write)\n\n[1] 9.478586\n\nsummary(hsb2$read)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  28.00   44.00   50.00   52.23   60.00   76.00 \n\nsd(hsb2$read)\n\n[1] 10.25294\n\n\n\nNow our categorical variables!\nWe can see that for socioeconomic status for students, the majority of them, 47.5%, are in the middle class. For school type, most of the senior students, 84%, are in public school. And finally for program type, most seniors, 52.5% are in academic program.\n\n\nhsb2 %&gt;% count(ses) %&gt;% mutate(percentage = n/sum(n)*100)\n\n     ses  n percentage\n1   high 58       29.0\n2    low 47       23.5\n3 middle 95       47.5\n\nhsb2 %&gt;% count(schtyp) %&gt;% mutate(percentage = n/sum(n)*100)\n\n   schtyp   n percentage\n1 private  32         16\n2  public 168         84\n\nhsb2 %&gt;% count(prog) %&gt;% mutate(percentage = n/sum(n)*100)\n\n        prog   n percentage\n1   academic 105       52.5\n2    general  45       22.5\n3 vocational  50       25.0\n\n\n\nProvide any visualizations that are relevant to your research question. Chapter 2 in this online text is a good resource for creating different plots.\n\nThe choice of visualization can be highly dependent on your question, but some ideas are:\n\nA visual summary of your (quantitative) outcome of interest:\n\nExamples: a density plot, a box plot, a histogram, a relative frequency histogram, etc.\n\nA visual summary of your predictor(s) of interest:\n\nFor one quantitative predictor, the examples in the previous bullet point apply\nFor one categorical predictor, you could do a bar chart, mosaic plot, etc.\n\nA visual summary of the relationship between one quantitative variable and one or more different variables:\n\nIdeas: scatter plots (maybe with points colored by a category of some categorical variable), side-by-side histograms, side-by-side box plots, side-by-side histograms, overlaid density plots, etc.\n\n\n\nWe have already seen some plots of our data in our data cleaning section but I will also be providing a bit more to visualize our data and the relationships between each other. Starting with our response variable, standardized writing scores.\n\n\n\nggplot(hsb2, aes(y=write)) + geom_boxplot(fill = \"lightblue\") + labs(title = \"Standardized Writing Scores\", y = \"Count\")+ theme_classic()\n\n\n\n\n\n\n\n\n\nEarlier in the data cleaning, we showed data visualization for each variable, now let us compare them to our response variable.\n\n\nggplot(hsb2, aes(x = ses,y=write)) + geom_boxplot(fill = \"lightblue\") + labs( y= \"Writing Score\", x = \"Socioeconomic Status\")+ theme_classic()\n\n\n\n\n\n\n\nggplot(hsb2, aes(x = schtyp,y= write)) + geom_boxplot(fill = \"lightblue\") + labs(y = \"Writing Score\", x = \"School Type\")+ theme_classic()\n\n\n\n\n\n\n\nggplot(hsb2, aes(x = prog,y= write)) + geom_boxplot(fill = \"lightblue\") + labs(y = \"Writing Score\", x = \"Program Type\")+ theme_classic()"
  },
  {
    "objectID": "FP CP2_AHO.html#source",
    "href": "FP CP2_AHO.html#source",
    "title": "Final Project CP2",
    "section": "Source:",
    "text": "Source:\nNational Center for Education Statistics - Statistical Consulting. (n.d.). High School & Beyond (HS&B) - overview. High School & Beyond (HS&B) - Overview. https://nces.ed.gov/surveys/hsb/\nExploring Data with Graphic; R Learning Modules. UCLA: Statistical Consulting Group. from https://stats.oarc.ucla.edu/r/modules/exploring-data-with-graphics/ (accessed April 28, 2025)."
  },
  {
    "objectID": "Lab10_Lasso_Cont (1).html",
    "href": "Lab10_Lasso_Cont (1).html",
    "title": "Lab 10: LASSO (Continued)",
    "section": "",
    "text": "Getting Started\n\n\n\n\n\n\nDownload the .qmd file from Moodle and any needed .xlsx or .csv data files. Save these in the same folder/directory.\nOpen the Quarto file in RStudio: File &gt; Open File... &gt;. If you’re working on the MHC RStudio server, you need to upload the files first: go to the Files panel, then click Upload. Upload the .qmd file and any data files. You will need to upload each file one at a time.\nUpdate the author and date in the YAML header of this file.\nClick the Render button. If successful, you should have a new window pop up with a nice looking HTML document.\nFor this lab, you may need to still the package glmnet.\n\nAsk for help if you encounter issues on any of the steps above. Once you’ve successfully made it through these steps, you can continue."
  },
  {
    "objectID": "Lab10_Lasso_Cont (1).html#build-the-model-for-a-range-of-tuning-parameter-values",
    "href": "Lab10_Lasso_Cont (1).html#build-the-model-for-a-range-of-tuning-parameter-values",
    "title": "Lab 10: LASSO (Continued)",
    "section": "Build the Model for a Range of Tuning Parameter Values",
    "text": "Build the Model for a Range of Tuning Parameter Values\n\nsample_data = read.csv(\"health_data_updated.csv\")\n# STEP 1: LASSO Model Specification\nlasso_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"glmnet\") %&gt;% \n  set_args(mixture = 1, penalty = tune())\n\nSTEP 1 Notes:\n\nWe use the glmnet, not lm, engine to build the LASSO.\nThe glmnet engine requires us to specify some arguments (set_args):\n\nmixture = 1 indicates LASSO. Changing this would run a different regularization algorithm.\npenalty = tune() indicates that we don’t (yet) know an appropriate \\(\\lambda\\) penalty term. We need to tune it.\n\n\nSuppose we want to build a model of response variable y using all possible predictors in a data frame sample_data.\n\n# STEP 2: Variable Recipe\nvariable_recipe &lt;- recipe(height ~ ., data = sample_data) %&gt;% \n  step_dummy(all_nominal_predictors())\n\nSTEP 2 Notes:\n\ny ~ . is shorthand for “y as a function of all other variables”\nThe function step_dummy() turns all potentially categorical (sometimes called nominal) predictors into indicator variables (which are unfortunately called “dummy variables” in some circles, hence the terrible name)\n\n\n# STEP 3: Workflow Specification (Model + Recipe)\nlasso_workflow &lt;- workflow() %&gt;% \n  add_recipe(variable_recipe) %&gt;% \n  add_model(lasso_spec)\n\nSTEP 3 Notes:\n\nThe function add_recipe includes our variable_recipe created in Step 2, which is specifying what predictors/response variables we have and what data set those variables belong to\nThe function add_model includes our lasso_spec created in Step 1, which is specifying what kind of model we wish to use (in this case, regression with LASSO)\n\n\n# STEP 4: Estimate Multiple LASSO Models Using a Range of Possible Lambda Values\nset.seed(123)\nlasso_models &lt;- lasso_workflow %&gt;% \n  tune_grid(\n    grid = grid_regular(penalty(range = c(-5, 1)), levels = 50),\n    resamples = vfold_cv(sample_data, v = 10),\n    metrics = metric_set(mae)\n  )\n\nSTEP 4 Notes:\n\nSince the CV process is random, we need to set.seed(___).\nWe use tune_grid() instead of fit() since we have to build multiple LASSO models, each using a different tuning parameter.\nThe function grid specifies the values of tuning parameter \\(\\lambda\\) that we want to try.\n\npenalty(range = c(___, ___)) specifies a range of \\(\\lambda\\) values we want to try, on the log10 scale.\nYou might start with c(-5, 1), which uses the range \\(\\lambda\\) from 0.00001 (\\(10^(-5)\\)) to 10 (\\(10^1\\)), and adjust from there.\nThe function levels is the number of \\(\\lambda\\) values to try in that range, thus how many LASSO models to build.\n\nThe functionsresamples and metrics indicate that we want to calculate a CV MAE (since mae is in metric_set) for each LASSO model. The number of folds in our cross-validation is given by v."
  },
  {
    "objectID": "Lab10_Lasso_Cont (1).html#tuning-lambda",
    "href": "Lab10_Lasso_Cont (1).html#tuning-lambda",
    "title": "Lab 10: LASSO (Continued)",
    "section": "Tuning \\(\\lambda\\)",
    "text": "Tuning \\(\\lambda\\)\n\nPlotting CV MAE (Y-Axis) for the LASSO Model for Each \\(\\lambda\\) (X-axis)\n\n# Calculate CV MAE for each LASSO model\nlasso_models %&gt;% collect_metrics()\n\n# Plotting option 1: plot lambda on log10 scale\nautoplot(lasso_models) + scale_x_log10()\n\n# Plotting option 2: plot lambda on original scale\nautoplot(lasso_models) + \n  scale_x_continuous() +\n  xlab(expression(lambda))\n\n# Plotting option 3: CV MAE (y-axis) with error bars (+/- 1 standard error)\n# with lambda (x-axis) on original scale\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda)) + \n  geom_errorbar(data = collect_metrics(lasso_models),\n                aes(x = penalty, ymin = mean - std_err, ymax = mean + std_err), \n                alpha = 0.5)\n\n\n# Identify lambda which produced the lowest (\"best\") CV MAE\nbest_penalty &lt;- lasso_models %&gt;% \n  select_best(metric = \"mae\")\nbest_penalty\n\n# Identify the largest lambda for which the CV MAE is\n# larger but \"roughly as good\" (within one standard error of the lowest)\nparsimonious_penalty &lt;- lasso_models %&gt;% \n  select_by_one_std_err(metric = \"mae\", desc(penalty))\nparsimonious_penalty\n\n\n\nFinalizing the “Best” LASSO Model\n\n# Parameters = final lambda value (best_penalty or parsimonious_penalty)\nfinal_lasso_model &lt;- lasso_workflow %&gt;% \n  finalize_workflow(parameters = parsimonious_penalty) %&gt;% \n  fit(data = sample_data)\n\n# Check it out\nfinal_lasso_model %&gt;% tidy()\n\n\n\nUsing Final LASSO Model to Make Predictions\n\nnew_data &lt;- data.frame(\n  age = 22, weight = 120, neck = 30, \n  chest = 115, abdomen = 105, hip = 101, \n  thigh = 55, knee = 40, ankle = 23, \n  biceps = 32, forearm = 29, wrist = 30\n)\nfinal_lasso_model %&gt;% \n  predict(new_data)\n\n\n\nVisualizing Shrinkage\nThis code can help us visualize shrinkage by comparing LASSO coefficients under each \\(\\lambda\\).\n\n# Get output for each LASSO\nall_lassos &lt;- final_lasso_model %&gt;% \n  extract_fit_parsnip() %&gt;%\n  pluck(\"fit\")\n\n# Plot coefficient paths as a function of lambda\nplot(all_lassos, xvar = \"lambda\", label = TRUE, col = rainbow(20))\n\n# Codebook for which variables the numbers correspond to\nrownames(all_lassos$beta)\n\n# For example, what are variables 2 and 4?\nrownames(all_lassos$beta)[c(2,4)]"
  },
  {
    "objectID": "Lab10_Lasso_Cont (1).html#context",
    "href": "Lab10_Lasso_Cont (1).html#context",
    "title": "Lab 10: LASSO (Continued)",
    "section": "Context",
    "text": "Context\n\nWorld = supervised learning\nWe want to model some output variable \\(Y\\) using a set of potential predictors (\\(X_1, X_2, ..., X_p\\)).\nTask = regression\n\\(y\\) is quantitative\nModel = linear regression\nWe’ll assume that the relationship between \\(Y\\) and (\\(X_1, X_2, ..., X_p\\)) can be represented by the model equation\n\\[\\mathbb{E}(Y \\mid X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p\\]\nand corresponding regression equation\n\\[\\hat{Y} = \\hat\\beta_0 + \\hat\\beta_1 X_1 + \\cdots + \\hat\\beta_p X_p \\]\nEstimation algorithm = LASSO"
  },
  {
    "objectID": "Lab10_Lasso_Cont (1).html#lasso-least-absolute-shrinkage-and-selection-operator",
    "href": "Lab10_Lasso_Cont (1).html#lasso-least-absolute-shrinkage-and-selection-operator",
    "title": "Lab 10: LASSO (Continued)",
    "section": "LASSO: Least Absolute Shrinkage and Selection Operator",
    "text": "LASSO: Least Absolute Shrinkage and Selection Operator\nIdea\nPenalize a predictor for adding complexity to the model (by penalizing its coefficient). Then track whether the predictor’s contribution to the model (lowering RSS) is enough to offset this penalty.\nCriterion\nIdentify the model coefficients \\(\\hat{\\beta}_1, \\hat{\\beta}_2, ...  \\hat{\\beta}_p\\) that minimize the penalized residual sum of squares:\n\\[SSR + \\lambda \\sum_{j=1}^p \\vert \\hat{\\beta}_j\\vert = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 + \\lambda \\sum_{j=1}^p \\vert \\hat{\\beta}_j\\vert\\]\nwhere\n\nsum of squared residuals (SSR) measures the overall model prediction error\nthe penalty term measures the overall size of the model coefficients\n\\(\\lambda \\ge 0\\) (“lambda”) is a tuning parameter\n\nPicking \\(\\lambda\\)\nWe cannot know the “best” value for \\(\\lambda\\) in advance. This varies from analysis to analysis.\nWe must try a reasonable range of possible values for \\(\\lambda\\). This also varies from analysis to analysis.\nIn general, we have to use trial-and-error to identify a range that is…\n\nwide enough that it doesn’t miss the “best” values for \\(\\lambda\\)\nnarrow enough that it focuses on reasonable values for \\(\\lambda\\)\n\nWe will explore what this means in more detail in one of the exercises."
  },
  {
    "objectID": "Lab10_Lasso_Cont (1).html#exercises",
    "href": "Lab10_Lasso_Cont (1).html#exercises",
    "title": "Lab 10: LASSO (Continued)",
    "section": "Exercises",
    "text": "Exercises\nWe will use the LASSO algorithm to help us build a good predictive model of height using the collection of 12 possible predictors in the new_health_data data set:\n\n\nError in library(tidyverse): there is no package called 'tidyverse'\n\n\nLet’s implement the LASSO. We’ll pause and adjust the code from the R Code Notes for LASSO section.\n\n# STEP 1: LASSO Algorithm & Model Specification\n# This is copied exactly from the R Code Notes for LASSO Section\nlasso_spec &lt;- linear_reg() %&gt;%             \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"glmnet\") %&gt;%                 \n  set_args(mixture = 1, penalty = tune()) \n\n\n# STEP 2: Variable Recipe\n# y ~. for response y \nvariable_recipe &lt;- recipe(height ~ ., data = new_health_data) %&gt;% \n  step_dummy(all_nominal_predictors())\n\n\n# STEP 3: Workflow Specification (Model + Recipe)\n# This is copied exactly from the R Code Notes for LASSO Section\nlasso_workflow &lt;- workflow() %&gt;% \n  add_recipe(variable_recipe) %&gt;% \n  add_model(lasso_spec)\n\n\n# STEP 4: Estimate 50 LASSO models using \n# lambda values on a \"grid\" or range from 10^(-5) to 10^(-0.1).\n# Calculate the 10-fold CV MAE for each of the 50 models.\n# Note: I usually start with a range from 10^(-5) to 10^1 and \n# tweak through trial-and-error.\nset.seed(123)\nlasso_models &lt;- lasso_workflow %&gt;% \n  tune_grid(\n    grid = grid_regular(penalty(range = c(-5, .1)), levels = 50),\n    resamples = vfold_cv(new_health_data, v = 10),\n    metrics = metric_set(mae)\n  )\n\nLast time: The code below finds the “best” \\(\\lambda\\) (the \\(\\lambda\\) for which the CV MAE is smallest).\n\n# This is copied exactly from the R Code Notes for LASSO Section\nbest_penalty &lt;- lasso_models %&gt;% \n  select_best(metric = \"mae\")\nbest_penalty\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1  0.0346 Preprocessor1_Model35\n\n\n\n# This is copied exactly from the R Code Notes for LASSO Section\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda))\n\n\n\n\n\n\n\n\n\nParsimonious Model\nSuppose we prefer a parsimonious model.\nThe “parsimonious model” is the one with the fewest necessary variables while still maintaining predictive accuracy.\nIn our case, we will define it as the model with the largest possible \\(\\lambda\\) (i.e., biggest penalty for adding new variables) that still has a CV MAE within 1 standard error of the “best” model (the model whose \\(\\lambda\\) gives the lowest CV MAE).\nThe plot below adds error bars to the CV MAE estimates of prediction error (+/- one standard error). Any model with a CV MAE that falls within another model’s error bars is not significantly better or worse at prediction:\n\n# This is copied exactly from the R Code Notes for LASSO Section\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda)) + \n  geom_errorbar(data = collect_metrics(lasso_models),\n                aes(x = penalty, ymin = mean - std_err, ymax = mean + std_err),\n                alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\nUse the plot with error bars to approximate the largest \\(\\lambda\\), thus the most simple LASSO model, that produces a CV MAE that’s within 1 standard error of the best model (thus is not significantly worse).\n\nThus we see that the lambda within 1 standard error is 0.1146\n\nCheck your approximation with code.\n\n# This is copied exactly from the R Code Notes for LASSO Section\nparsimonious_penalty &lt;- lasso_models %&gt;% \n  select_by_one_std_err(metric = \"mae\", desc(penalty))\nparsimonious_penalty\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1   0.115 Preprocessor1_Model40\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nMoving forward, we’ll use the parsimonious LASSO model. Simply report the tuning parameter \\(\\lambda\\) here.\n\nThe parsimonious lambda lasson model shows it is 0.1146.\n\n\n\n\n\nPicking a Range to Try for \\(\\lambda\\)\nThe range of values we tried for \\(\\lambda\\) had the following nice properties. (If it didn’t, we should adjust our range (make it narrower or wider).\n\nOur range was wide enough.\n\nThe “best” and “parsimonious” \\(\\lambda\\) values were not at the edges of the range, suggesting there aren’t better \\(\\lambda\\) values outside our range.\n\nOur range was narrow enough.\n\nWe didn’t observe any loooooong flat lines in CV MAE. Thus, we narrowed in on the \\(\\lambda\\) values where the “action is happening”, i.e. where changing \\(\\lambda\\) impacts the model.\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nModify your previous code to start with \\(\\lambda\\) in the range 10^(-5) to 10^(1) and see what you observe. Adjust the range until it seems appropriate (no flat lines, but no minimum on a boundary).\n\nI changed it and I saw the best lambda also changed since it was not at the boundary.\n\n\n\nLet’s finalize our parsimonious LASSO model:\n\nfinal_lasso &lt;- lasso_workflow %&gt;% \n  finalize_workflow(parameters = parsimonious_penalty) %&gt;% \n  fit(data = new_health_data)\n\nfinal_lasso %&gt;% \n  tidy()\n\n# A tibble: 13 × 3\n   term        estimate penalty\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept) 74.3       0.115\n 2 age         -0.00234   0.115\n 3 weight       0.124     0.115\n 4 neck        -0.181     0.115\n 5 chest        0         0.115\n 6 abdomen     -0.190     0.115\n 7 hip          0         0.115\n 8 thigh       -0.136     0.115\n 9 knee         0.161     0.115\n10 ankle        0.0230    0.115\n11 biceps       0         0.115\n12 forearm      0         0.115\n13 wrist        0         0.115\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nLet’s finalize our LASSO model.\n\nHow many and which predictors were kept in this model?\n\n\nWe keep 7 predictors which are age, weight, neck, abodomen, thigh, knee , and ankle.\n\n\nThrough shrinkage, the LASSO coefficients(the \\(\\hat\\beta_i\\)) lose some contextual meaning, so we typically shouldn’t interpret them. Why? THINK: What is the goal of LASSO modeling?\n\nSince LASSO shrinks coeffiencients, we can see which variables or beta values are biased in order to keep or not keep them. The goal of LASSO is to predict which predictors are the best for the model.\n\n\n\n\nOur parsimonious LASSO selected only 7 of the 12 possible predictors. Out of curiosity, how many predictors would have remained if we had used the best_penalty value for \\(\\lambda\\)?\n\nlasso_workflow %&gt;% \n  finalize_workflow(parameters = best_penalty) %&gt;% \n  fit(data = new_health_data) %&gt;% \n  tidy() %&gt;% \n  filter(estimate != 0)\n\n# A tibble: 11 × 3\n   term        estimate penalty\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)  94.4     0.0346\n 2 age          -0.0172  0.0346\n 3 weight        0.209   0.0346\n 4 neck         -0.490   0.0346\n 5 chest        -0.0707  0.0346\n 6 abdomen      -0.206   0.0346\n 7 thigh        -0.283   0.0346\n 8 knee          0.162   0.0346\n 9 biceps       -0.111   0.0346\n10 forearm      -0.0418  0.0346\n11 wrist        -0.0186  0.0346\n\n\nBased on this example, do you think LASSO is a greedy algorithm? Are you “stuck” with your past locally optimal choices? Compare the predictors in this larger model with those in the smaller, parsimonious model.\n\nYes, LASSO is a greedy algorithm because it checks all options. Yes, there is times where you can get “stuck” with the current optimization and not see better options."
  },
  {
    "objectID": "Lab10_Lasso_Cont (1).html#lasso-vs-least-squares",
    "href": "Lab10_Lasso_Cont (1).html#lasso-vs-least-squares",
    "title": "Lab 10: LASSO (Continued)",
    "section": "LASSO vs Least Squares",
    "text": "LASSO vs Least Squares\nLet’s compare our final_lasso model to the least squares model using all predictors.\n\n# Build the LS model\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\nls_workflow &lt;- workflow() %&gt;%\n  add_model(lm_spec) %&gt;%\n  add_recipe(variable_recipe)\n\nls_model &lt;- ls_workflow %&gt;% \n  fit(data = new_health_data) \n\n# examine coefficients\nls_model %&gt;% \n  tidy()\n\n# A tibble: 13 × 5\n   term        estimate std.error statistic     p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n 1 (Intercept) 110.       16.9       6.51   0.000000561\n 2 age          -0.0234    0.0367   -0.637  0.529      \n 3 weight        0.266     0.0573    4.64   0.0000810  \n 4 neck         -0.671     0.335    -2.00   0.0556     \n 5 chest        -0.119     0.131    -0.908  0.372      \n 6 abdomen      -0.196     0.113    -1.73   0.0946     \n 7 hip          -0.0978    0.189    -0.518  0.609      \n 8 thigh        -0.313     0.163    -1.92   0.0661     \n 9 knee          0.199     0.272     0.733  0.470      \n10 ankle        -0.0262    0.449    -0.0583 0.954      \n11 biceps       -0.168     0.200    -0.837  0.410      \n12 forearm      -0.0770    0.144    -0.536  0.596      \n13 wrist        -0.0962    0.647    -0.149  0.883      \n\n# get 10-fold CV MAE\nset.seed(123)\nls_workflow %&gt;% \n  fit_resamples(\n    resamples = vfold_cv(new_health_data,v = 10),\n    metrics = metric_set(mae)\n  ) %&gt;% \n  collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    1.59    10   0.221 Preprocessor1_Model1\n\n\n\nOur final_lasso has ___ predictors and a CV MAE of ___ (calculated above). The ls_model has 12 predictors and a CV MAE of ___ Comment.\n\n\nOur final has 7 predictors, and CV of 1.59 and the ls model has 12 predictors with MAE of 2.63?\n\n\nUse both final_lasso and ls_model to predict the height of the new patient below. How do these compare? Does this add to or calm any fears you might have had about shrinking coefficients?!\n\n\nnew_patient &lt;- data.frame(age = 50, weight = 200, neck = 40, \n                          chest = 115, abdomen = 105, hip = 100, \n                          thigh = 60, knee = 38, ankle = 23, biceps = 32, \n                          forearm = 29, wrist = 19) \n\n\n# LS prediction\npredict(ls_model, new_data = new_patient)\n\n\n# LASSO prediction\npredict(final_lasso, new_data = new_patient)\n\n\nWhich final model would you choose, the LASSO or least squares?\n\n\nThe values are super close which means both methods work but I prefer LASSO as it is able to look at more optimal combinations."
  },
  {
    "objectID": "Test.html",
    "href": "Test.html",
    "title": "STAT 244-SC",
    "section": "",
    "text": "Here I will be talking more about my Final Project in my class, stat-244-sc! I am doing a Final Project in collaboration with Hjordis Aho.\nFor our final project, we choose HSB2 dataset which is a dataset containing 200 randomly sampled responses from a survey conducted by the National Center of Education Statsistics. The survey titled “High School and Beyond” was conducted on high school seniors.\nWe will be researching students’ standardized writing scores and looking at four predictor variables which are: standardized reading score, socio economic status of student’s family, type of school, and type of program they are in.\nHere are some data visualization of our predictor variables and how they compare with our response variable.\n\nlibrary(ggplot2)\nhsb2=read.csv(\"hsb2.csv\")\nhead(hsb2)\n\n   id gender  race    ses schtyp       prog read write math science socst\n1  70   male white    low public    general   57    52   41      47    57\n2 121 female white middle public vocational   68    59   53      63    61\n3  86   male white   high public    general   44    33   54      58    31\n4 141   male white   high public vocational   63    44   47      53    56\n5 172   male white middle public   academic   47    52   57      53    61\n6 113   male white middle public   academic   44    52   51      63    61\n\nggplot(hsb2, aes(x = read,y=write)) + geom_line(col = \"lightblue\") + labs( y= \"Writing Score\", x = \"Reading Score\")+ theme_classic()\n\n\n\n\n\n\n\nggplot(hsb2, aes(x = ses,y=write)) + geom_boxplot(fill = \"lightblue\") + labs( y= \"Writing Score\", x = \"Socioeconomic Status\")+ theme_classic()\n\n\n\n\n\n\n\nggplot(hsb2, aes(x = schtyp,y= write)) + geom_boxplot(fill = \"lightblue\") + labs(y = \"Writing Score\", x = \"School Type\")+ theme_classic()\n\n\n\n\n\n\n\nggplot(hsb2, aes(x = prog,y= write)) + geom_boxplot(fill = \"lightblue\") + labs(y = \"Writing Score\", x = \"Program Type\")+ theme_classic()"
  }
]