[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my STAT244 Website!!",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Lab9_Lasso.html",
    "href": "Lab9_Lasso.html",
    "title": "Lab 9: LASSO",
    "section": "",
    "text": "Getting Started\n\n\n\n\n\n\nDownload the .qmd file from Moodle and any needed .xlsx or .csv data files. Save these in the same folder/directory.\nOpen the Quarto file in RStudio: File &gt; Open File... &gt;. If you’re working on the MHC RStudio server, you need to upload the files first: go to the Files panel, then click Upload. Upload the .qmd file and any data files. You will need to upload each file one at a time.\nUpdate the author and date in the YAML header of this file.\nClick the Render button. If successful, you should have a new window pop up with a nice looking HTML document.\nFor this lab, you may need to still the package glmnet.\n\nAsk for help if you encounter issues on any of the steps above. Once you’ve successfully made it through these steps, you can continue."
  },
  {
    "objectID": "Lab9_Lasso.html#build-the-model-for-a-range-of-tuning-parameter-values",
    "href": "Lab9_Lasso.html#build-the-model-for-a-range-of-tuning-parameter-values",
    "title": "Lab 9: LASSO",
    "section": "Build the Model for a Range of Tuning Parameter Values",
    "text": "Build the Model for a Range of Tuning Parameter Values\n\n# STEP 1: LASSO Model Specification\nlasso_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"glmnet\") %&gt;% \n  set_args(mixture = 1, penalty = tune())\n\nError in linear_reg() %&gt;% set_mode(\"regression\") %&gt;% set_engine(\"glmnet\") %&gt;% : could not find function \"%&gt;%\"\n\n\nSTEP 1 Notes:\n\nWe use the glmnet, not lm, engine to build the LASSO.\nThe glmnet engine requires us to specify some arguments (set_args):\n\nmixture = 1 indicates LASSO. Changing this would run a different regularization algorithm.\npenalty = tune() indicates that we don’t (yet) know an appropriate \\(\\lambda\\) penalty term. We need to tune it.\n\n\nSuppose we want to build a model of response variable y using all possible predictors in a data frame sample_data.\n\n# STEP 2: Variable Recipe\nvariable_recipe &lt;- recipe(height ~ ., data = health_data) %&gt;% \n  step_dummy(all_nominal_predictors())\n\nSTEP 2 Notes:\n\ny ~ . is shorthand for “y as a function of all other variables”\nThe function step_dummy() turns all potentially categorical (sometimes called nominal) predictors into indicator variables (which are unfortunately called “dummy variables” in some circles, hence the terrible name)\n\n\n# STEP 3: Workflow Specification (Model + Recipe)\nlasso_workflow &lt;- workflow() %&gt;% \n  add_recipe(variable_recipe) %&gt;% \n  add_model(lasso_spec)\n\nSTEP 3 Notes:\n\nThe function add_recipe includes our variable_recipe created in Step 2, which is specifying what predictors/response variables we have and what data set those variables belong to\nThe function add_model includes our lasso_spec created in Step 1, which is specifying what kind of model we wish to use (in this case, regression with LASSO)\n\n\n# STEP 4: Estimate Multiple LASSO Models Using a Range of Possible Lambda Values\nset.seed(123)\nlasso_models &lt;- lasso_workflow %&gt;% \n  tune_grid(\n    grid = grid_regular(penalty(range = c(-5, 1)), levels = 50),\n    resamples = vfold_cv(health_data, v = 10),\n    metrics = metric_set(mae)\n  )\n\nSTEP 4 Notes:\n\nSince the CV process is random, we need to set.seed(___).\nWe use tune_grid() instead of fit() since we have to build multiple LASSO models, each using a different tuning parameter.\nThe function grid specifies the values of tuning parameter \\(\\lambda\\) that we want to try.\n\npenalty(range = c(___, ___)) specifies a range of \\(\\lambda\\) values we want to try, on the log10 scale.\nYou might start with c(-5, 1), which uses the range \\(\\lambda\\) from 0.00001 (\\(10^(-5)\\)) to 10 (\\(10^1\\)), and adjust from there.\nThe function levels is the number of \\(\\lambda\\) values to try in that range, thus how many LASSO models to build.\n\nThe functionsresamples and metrics indicate that we want to calculate a CV MAE (since mae is in metric_set) for each LASSO model. The number of folds in our cross-validation is given by v."
  },
  {
    "objectID": "Lab9_Lasso.html#tuning-lambda",
    "href": "Lab9_Lasso.html#tuning-lambda",
    "title": "Lab 9: LASSO",
    "section": "Tuning \\(\\lambda\\)",
    "text": "Tuning \\(\\lambda\\)\n\nPlotting CV MAE (Y-Axis) for the LASSO Model for Each \\(\\lambda\\) (X-axis)\n\n# Calculate CV MAE for each LASSO model\nlasso_models %&gt;% collect_metrics()\n\n# Plotting option 1: plot lambda on log10 scale\nautoplot(lasso_models) + scale_x_log10()\n\n# Plotting option 2: plot lambda on original scale\nautoplot(lasso_models) + \n  scale_x_continuous() +\n  xlab(expression(lambda))\n\n# Plotting option 3: CV MAE (y-axis) with error bars (+/- 1 standard error)\n# with lambda (x-axis) on original scale\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda)) + \n  geom_errorbar(data = collect_metrics(lasso_models),\n                aes(x = penalty, ymin = mean - std_err, ymax = mean + std_err), \n                alpha = 0.5)\n\n\n# Identify lambda which produced the lowest (\"best\") CV MAE\nbest_penalty &lt;- lasso_models %&gt;% \n  select_best(metric = \"mae\")\nbest_penalty\n\n# Identify the largest lambda for which the CV MAE is\n# larger but \"roughly as good\" (within one standard error of the lowest)\nparsimonious_penalty &lt;- lasso_models %&gt;% \n  select_by_one_std_err(metric = \"mae\", desc(penalty))\nparsimonious_penalty\n\n\n\nFinalizing the “Best” LASSO Model\n\n# Parameters = final lambda value (best_penalty or parsimonious_penalty)\nfinal_lasso_model &lt;- lasso_workflow %&gt;% \n  finalize_workflow(parameters = ___) %&gt;% \n  fit(data = sample_data)\n\n# Check it out\nfinal_lasso_model %&gt;% tidy()\n\n\n\nUsing Final LASSO Model to Make Predictions\n\nfinal_lasso_model %&gt;% \n  predict(new_data = SOME DATA FRAME W/ OBSERVATIONS ON EACH PREDICTOR)\n\n\n\nVisualizing Shrinkage\nThis code can help us visualize shrinkage by comparing LASSO coefficients under each \\(\\lambda\\).\n\n# Get output for each LASSO\nall_lassos &lt;- final_lasso_model %&gt;% \n  extract_fit_parsnip() %&gt;%\n  pluck(\"fit\")\n\n# Plot coefficient paths as a function of lambda\nplot(all_lassos, xvar = \"lambda\", label = TRUE, col = rainbow(20))\n\n# Codebook for which variables the numbers correspond to\nrownames(all_lassos$beta)\n\n# For example, what are variables 2 and 4?\nrownames(all_lassos$beta)[c(2,4)]"
  },
  {
    "objectID": "Lab9_Lasso.html#lasso-least-absolute-shrinkage-and-selection-operator",
    "href": "Lab9_Lasso.html#lasso-least-absolute-shrinkage-and-selection-operator",
    "title": "Lab 9: LASSO",
    "section": "LASSO: least absolute shrinkage and selection operator",
    "text": "LASSO: least absolute shrinkage and selection operator\nIdea\nPenalize a predictor for adding complexity to the model (by penalizing its coefficient). Then track whether the predictor’s contribution to the model (lowering RSS) is enough to offset this penalty.\nCriterion\nIdentify the model coefficients \\(\\hat{\\beta}_1, \\hat{\\beta}_2, ...  \\hat{\\beta}_p\\) that minimize the penalized residual sum of squares:\n\\[SSR + \\lambda \\sum_{j=1}^p \\vert \\hat{\\beta}_j\\vert = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 + \\lambda \\sum_{j=1}^p \\vert \\hat{\\beta}_j\\vert\\]\nwhere\n\nsum of squared residuals (SSR) measures the overall model prediction error\nthe penalty term measures the overall size of the model coefficients\n\\(\\lambda \\ge 0\\) (“lambda”) is a tuning parameter\n\n\nCOMMENT: Picking \\(\\lambda\\)\nWe cannot know the “best” value for \\(\\lambda\\) in advance. This varies from analysis to analysis.\nWe must try a reasonable range of possible values for \\(\\lambda\\). This also varies from analysis to analysis.\nIn general, we have to use trial-and-error to identify a range that is…\n\nwide enough that it doesn’t miss the “best” values for \\(\\lambda\\)\nnarrow enough that it focuses on reasonable values for \\(\\lambda\\)\n\nWe will explore what this means in more detail in one of the exercises."
  },
  {
    "objectID": "Lab9_Lasso.html#exercise-1",
    "href": "Lab9_Lasso.html#exercise-1",
    "title": "Lab 9: LASSO",
    "section": "EXERCISE 1",
    "text": "EXERCISE 1\nLet’s compare the CV MAEs (y-axis) for our 50 LASSO models which used 50 different \\(\\lambda\\) values (x-axis):\n\n# This is copied exactly from the R Code Notes for LASSO Section\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda))\n\nError in autoplot(lasso_models): could not find function \"autoplot\"\n\n\n\nWe told R to use a range of \\(\\lambda\\) from -5 to -0.1 on the log10 scale. Calculate this range on the non-log scale and confirm that it matches the x-axis.\n\nWhat can we observe from this plot?\n\nANSWER."
  },
  {
    "objectID": "Lab9_Lasso.html#exercise-2",
    "href": "Lab9_Lasso.html#exercise-2",
    "title": "Lab 9: LASSO",
    "section": "EXERCISE 2",
    "text": "EXERCISE 2\n\nIn the plot above, roughly which value of the \\(\\lambda\\) penalty parameter produces the smallest CV MAE?\n\n\nANSWER.\n\nCheck your approximation with code.\n\n# This is copied exactly from the R Code Notes for LASSO Section\nbest_penalty &lt;- lasso_models %&gt;% \n  select_best(metric = \"mae\")\n\nError in lasso_models %&gt;% select_best(metric = \"mae\"): could not find function \"%&gt;%\"\n\nbest_penalty\n\nError: object 'best_penalty' not found\n\n\n\nSuppose we prefer a parsimonious model.\n\nThe “parsimonious model” is the one with the fewest necessary variables while still maintaining predictive accuracy.\nIn our case, we will define it as the model with the largest possible \\(\\lambda\\) (i.e., biggest penalty for adding new variables) that still has a CV MAE within 1 standard error of the “best” model (the model whose \\(\\lambda\\) gives the lowest CV MAE).\nThe plot below adds error bars to the CV MAE estimates of prediction error (+/- one standard error). Any model with a CV MAE that falls within another model’s error bars is not significantly better or worse at prediction:\n\n# This is copied exactly from the R Code Notes for LASSO Section\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda)) + \n  geom_errorbar(data = collect_metrics(lasso_models),\n                aes(x = penalty, ymin = mean - std_err, ymax = mean + std_err), alpha = 0.5)\n\nError in autoplot(lasso_models): could not find function \"autoplot\"\n\n\nUse this to approximate the largest \\(\\lambda\\), thus the most simple LASSO model, that produces a CV MAE that’s within 1 standard error of the best model (thus is not significantly worse).\n\nANSWER.\n\nCheck your approximation with code.\n\n# This is copied exactly from the R Code Notes for LASSO Section\nparsimonious_penalty &lt;- lasso_models %&gt;% \n  select_by_one_std_err(metric = \"mae\", desc(penalty))\n\nError in lasso_models %&gt;% select_by_one_std_err(metric = \"mae\", desc(penalty)): could not find function \"%&gt;%\"\n\nparsimonious_penalty\n\nError: object 'parsimonious_penalty' not found\n\n\n\nMoving forward, we’ll use the parsimonious LASSO model. Simply report the tuning parameter \\(\\lambda\\) here.\n\n\nANSWER.\n\n\nPicking a Range to Try for \\(\\lambda\\)\nThe range of values we tried for \\(\\lambda\\) had the following nice properties. (If it didn’t, we should adjust our range (make it narrower or wider).\n\nOur range was wide enough.\n\nThe “best” and “parsimonious” \\(\\lambda\\) values were not at the edges of the range, suggesting there aren’t better \\(\\lambda\\) values outside our range.\n\nOur range was narrow enough.\n\nWe didn’t observe any loooooong flat lines in CV MAE. Thus, we narrowed in on the \\(\\lambda\\) values where the “action is happening”, i.e. where changing \\(\\lambda\\) impacts the model.\n\n\n\n\nEXERCISE 3\nModify your previous code to start with \\(\\lambda\\) in the range 10^(-5) to 10^(-0.1) and see what you observe.\n\nANSWER"
  },
  {
    "objectID": "Lab9_Lasso.html#exercise-4-finalizing-our-lasso-model",
    "href": "Lab9_Lasso.html#exercise-4-finalizing-our-lasso-model",
    "title": "Lab 9: LASSO",
    "section": "EXERCISE 4: Finalizing our LASSO model",
    "text": "EXERCISE 4: Finalizing our LASSO model\nLet’s finalize our parsimonious LASSO model:\n\nfinal_lasso &lt;- lasso_workflow %&gt;% \n  finalize_workflow(parameters = parsimonious_penalty) %&gt;% \n  fit(data = health_data)\n\nError in lasso_workflow %&gt;% finalize_workflow(parameters = parsimonious_penalty) %&gt;% : could not find function \"%&gt;%\"\n\nfinal_lasso %&gt;% \n  tidy()\n\nError in final_lasso %&gt;% tidy(): could not find function \"%&gt;%\"\n\n\n\nHow many and which predictors were kept in this model?\n\n\nANSWER.\n\n\nThrough shrinkage, the LASSO coefficients(the \\(\\hat\\beta_i\\)) lose some contextual meaning, so we typically shouldn’t interpret them. Why? THINK: What is the goal of LASSO modeling?\n\n\nANSWER."
  },
  {
    "objectID": "Lab8_VariableSubsetSelection.html",
    "href": "Lab8_VariableSubsetSelection.html",
    "title": "Variable Subset Selection",
    "section": "",
    "text": "Best Subset Selection (Notes)\n\nWith \\(p\\) predictors, there are \\(2^p\\) possible models because there are \\(2^p\\) possible subsets of the \\(p\\) predictors. (Size of the powerset) – alternatively, two choices for each predictor, either include or not include\nIn best subset selection procedure, we fit all \\(2^p\\) models and choose the one with the best value of our chosen evaluation metric (e.g., test error estimate based on CV, using some dedicated test set, etc.). Our “evaluation metric” (equivalently, “error metric”) should fairly assess test performance of model (as opposed to training set performance)\n\\(2^p\\) models? This sounds expensive.\nExample: \\(p = 4\\), 16 potential models to fit (including the zero variable model, i.e., one with just an intercept)\nDoesn’t try to transform variables or include interaction terms – would have to manually include transformed variables or interaction models in model\n\n\n\nForward Stepwise Selection\n\nIdea: add variables one at a time, choosing the “best” variable each time\n\nThis is known as a greedy algorithm: goes for temporarily/locally best choice without thinking about some kind of long-term optimum\n\nHistorical note: many past (and current) implementations to determine which variable is “best” is to choose the one whose inclusion had the lowest p-value. Problematic: discussed at the end.\n\nBetter to use CV estimates of test error instead of choices based on p-values\n\n\n\n\nGeneral Notes\n\nBackwards and forwards selection can give different results due to greedy behavior of each\nWhen several “reasonable” methods (e.g., forward/backward selection if best subset is prohibitively expensive), best practice is to try all methods, compare their results, and report all results\n\n\n\nCautions\n\nSome machine learning practitioners don’t like automated selection methods, b/c encourage us to not think about the variables b/c can just dump data into an algorithm and get a model\nNeed to think of variables in context, and carelessly using automated procedures can do real harm\nUsing p-values for deciding which variables to add/remove results gives unstable (and often “undesireable”) results when several quantitative predictors are correlated with one another (called collinearity – e.g., one predictor is a linear combination of others) – not the same as a violated independence assumption\n\nStatistical inference problems on final selected model: chosen through multiple hypothesis testing * Because we come to select a final model by trying a large number of models, multiple hypothesis testing is a big issue.\n\nWith multiple testing, the idea is that testing many hypotheses runs the risk of a result being statistically significant just by chance.\nMight have to do a Bonferroni correction or something like that (ignore this comment if you haven’t taken STAT 242)\n\nCIs are misleadingly narrow (from multiple hyp. testing)\nP-values are misleadingly small (from mult. hyp testing)\n\n\n\n\nSummary\nSubset selection methods use model quality metrics to search through different models in an automated way\n\nBest subset selection: intuitive, accurate, quickly computationally expensive\nStepwise selection (either forward or backwards): faster but not guaranteed to find “best model” due to greedy nature\nNo methods automatically consider transformations or interactions (we manually include those to exlore them)\nProblems with statistical inference\n\nShrinkage/regularization methods are a popular alternative to subset selection.\n\n\nFor Fun: Heat Map for Exploring Multicollinearity\n\n# Load packages\nlibrary(tidyverse)\n\nError in library(tidyverse): there is no package called 'tidyverse'\n\nlibrary(tidymodels)\n\nError in library(tidymodels): there is no package called 'tidymodels'\n\nlibrary(readxl)\n\nError in library(readxl): there is no package called 'readxl'\n\n# Load data\nhealth_data = read_xlsx(\"healthdata.xlsx\")\n\nError in read_xlsx(\"healthdata.xlsx\"): could not find function \"read_xlsx\"\n\nhead(health_data)\n\nError: object 'health_data' not found\n\n\n\n# Get the correlation matrix\nlibrary(reshape2)\n\nError in library(reshape2): there is no package called 'reshape2'\n\ncor_matrix &lt;- cor(health_data)\n\nError: object 'health_data' not found\n\ncor_matrix[lower.tri(cor_matrix)] &lt;- NA\n\nError: object 'cor_matrix' not found\n\ncor_matrix &lt;- cor_matrix %&gt;% \n  melt() %&gt;% \n  na.omit() %&gt;% \n  rename(correlation = value)\n\nError in cor_matrix %&gt;% melt() %&gt;% na.omit() %&gt;% rename(correlation = value): could not find function \"%&gt;%\"\n\n# Visualize the correlation for each pair of variables\nggplot(cor_matrix, aes(x = Var1, y = Var2, fill = correlation)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(\n    low = \"blue\", high = \"red\", mid = \"white\", \n    midpoint = 0, limit = c(-1,1)) +\n  labs(x = \"\", y = \"\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + \n  coord_fixed()\n\nError in ggplot(cor_matrix, aes(x = Var1, y = Var2, fill = correlation)): could not find function \"ggplot\"\n\n\n\n# STEP 1: Model Specification\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\nError in linear_reg() %&gt;% set_mode(\"regression\") %&gt;% set_engine(\"lm\"): could not find function \"%&gt;%\"\n\n# STEP 2: Model estimation\nheight_model_1 &lt;- lm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, data = health_data)\n\nError in lm_spec %&gt;% fit(height ~ age + weight + neck + chest + abdomen + : could not find function \"%&gt;%\"\n\n# Look at model\nheight_model_1 %&gt;% tidy()\n\nError in height_model_1 %&gt;% tidy(): could not find function \"%&gt;%\"\n\n\n\n\nBest Subset Selection Algorithm\n\nBuild all possible models that use any combination of the available predictors\nIdentify the best model with respect to some chosen metric (eg: CV MAE, CV MSE) and context.\n\n\n\nExercise\nSuppose we used this algorithm for our height model with all 12 possible predictors (13 variables total, one is height, leaving 12 predictors). What’s the main drawback?\n\nANSWER.\n\n\n\nBackward Stepwise Selection Algorithm\n\nBuild a model with all \\(p\\) possible predictors,\nRepeat the following until only 1 predictor remains in the model:\n\nRemove the 1 predictor that increases the MSE/MAE by the least\nBuild a model with the remaining predictors.\n\n\nYou now have \\(p\\) competing models: one with all \\(p\\) predictors, one with \\(p-1\\) predictors, …, and one with 1 predictor. In a future HW assignment, you will implement the above using MSE/MAE error metrics.\nFor this example though, for simplicity, we will identify the “best” model with respect to p-values.\nLet’s try out the first few steps!\nFirst, we would compute the 10-fold MAE of the model with all 12 predictors.\n\nset.seed(244)\nmodel_12_predictors_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n   height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n    resamples = vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae)\n  )\n\nError in lm_spec %&gt;% fit_resamples(height ~ age + weight + neck + chest + : could not find function \"%&gt;%\"\n\nmodel_12_predictors_cv %&gt;%  collect_metrics()\n\nError in model_12_predictors_cv %&gt;% collect_metrics(): could not find function \"%&gt;%\"\n\n\nThe model with all 12 predictors has a MAE of about 5.55.\nThen let’s look at which predictor to remove to identify the “best” model with 11 predictors.\n\n# Original model with 12 predictors\n# Find the \"least significant\" predictor\nlm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n      data = health_data) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4))\n\nError in lm_spec %&gt;% fit(height ~ age + weight + neck + chest + abdomen + : could not find function \"%&gt;%\"\n\n\nLooks like we should remove biceps as predictor, bc it appears to be the least statisticlly significant!\nWe would then compute the 10-fold MAE of the model with 11 predictors.\n\n#COMPUTE MAE OF MODEL WITH 11 PREDICTORS SINCE BICEPS WAS REMOVED\nset.seed(244)\nmodel_11_predictors_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n   height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle  + forearm + wrist,\n    resamples = vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae)\n  )\n\nError in lm_spec %&gt;% fit_resamples(height ~ age + weight + neck + chest + : could not find function \"%&gt;%\"\n\nmodel_11_predictors_cv %&gt;%  collect_metrics()\n\nError in model_11_predictors_cv %&gt;% collect_metrics(): could not find function \"%&gt;%\"\n\n\nWith 11 predictors, we can see that our MAE is 5.55, the full number shows a slight difference.\n\n# 11 predictors\n# UPDATE this code\nlm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + forearm + wrist,\n      data = health_data) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4))\n\nError in lm_spec %&gt;% fit(height ~ age + weight + neck + chest + abdomen + : could not find function \"%&gt;%\"\n\n\nLooks like we should remove ‘neck’ becayse it is the least statistically significant (p = 0.8880)\n\nset.seed(244)\nmodel_10_predictors_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n   height ~ age + weight  + chest + abdomen + hip + thigh + knee + ankle  + forearm + wrist,\n    resamples = vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae)\n  )\n\nError in lm_spec %&gt;% fit_resamples(height ~ age + weight + chest + abdomen + : could not find function \"%&gt;%\"\n\nmodel_10_predictors_cv %&gt;%  collect_metrics()\n\nError in model_10_predictors_cv %&gt;% collect_metrics(): could not find function \"%&gt;%\"\n\n\nNow when we removed neck and only have 10 predictors, we see that our MAE is 5.51!\n\n# 10 predictors\n# UPDATE this code by removing neck\nlm_spec %&gt;% \n  fit(height ~ age + weight  + chest + abdomen + hip + thigh + knee + ankle  + forearm + wrist,\n      data = health_data) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4))\n\nError in lm_spec %&gt;% fit(height ~ age + weight + chest + abdomen + hip + : could not find function \"%&gt;%\"\n\n\nNow from the results, we see that we should remove knee since the pvalue is 0.8497 because it is the most statistically insignificant.\n\nset.seed(244)\nmodel_9_predictors_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n   height ~ age + weight  + chest + abdomen + hip + thigh  + ankle  + forearm + wrist,\n    resamples = vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae)\n  )\n\nError in lm_spec %&gt;% fit_resamples(height ~ age + weight + chest + abdomen + : could not find function \"%&gt;%\"\n\nmodel_9_predictors_cv %&gt;%  collect_metrics()\n\nError in model_9_predictors_cv %&gt;% collect_metrics(): could not find function \"%&gt;%\"\n\n\n\n\nBackward Stepwise Selection Step-by-Step Results\nBelow is the complete model sequence along with 10-fold CV MAE for each model (using set.seed(244)).\n\n\n\npred\nCV MAE\npredictor list\n\n\n\n\n12\n5.555\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist, knee, neck, biceps\n\n\n11\n5.553\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist, knee, neck\n\n\n10\n5.512\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist, knee\n\n\n9\n5.425\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist\n\n\n8\n5.047\nweight, hip, forearm, thigh, chest, abdomen, age, ankle\n\n\n7\n5.013\nweight, hip, forearm, thigh, chest, abdomen, age\n\n\n6\n4.684\nweight, hip, forearm, thigh, chest, abdomen\n\n\n5\n4.460\nweight, hip, forearm, thigh, chest\n\n\n4\n4.386\nweight, hip, forearm, thigh\n\n\n3\n4.091\nweight, hip, forearm\n\n\n2\n3.733\nweight, hip\n\n\n1\n3.658\nweight"
  },
  {
    "objectID": "Intro-CV.html",
    "href": "Intro-CV.html",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "",
    "text": "Getting Started\n\n\n\n\n\n\nDownload the .qmd file from Moodle and any needed .xlsx or .csv data files. Save these in the same folder/directory.\nOpen the Quarto file in RStudio: File &gt; Open File... &gt;. If you’re working on the MHC RStudio server, you need to upload the files first: go to the Files panel, then click Upload. Upload the .qmd file and any data files. You will need to upload each file one at a time.\nUpdate the author and date in the YAML header of this file.\nClick the Render button. If successful, you should have a new window pop up with a nice looking HTML document.\n\nAsk for help if you encounter issues on any of the steps above. Once you’ve successfully made it through these steps, you can continue."
  },
  {
    "objectID": "Intro-CV.html#context",
    "href": "Intro-CV.html#context",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "Context",
    "text": "Context\n\nBroader subject area: supervised learning\nWe want to build a model for some output RV \\(Y\\) given various predictor RVs \\(X_1, \\ldots, X_p\\).\nTask: regression\n\\(Y\\) is quantitative (takes numerical values)\nAlgorithm: linear regression model\nWe’ll assume that the relationship between \\(Y\\) and \\(X\\) can be represented by\n\n\\[\\mathbb{E}(Y \\mid X_1, \\ldots, X_p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p  \\]"
  },
  {
    "objectID": "Intro-CV.html#review-k-fold-cross-validation",
    "href": "Intro-CV.html#review-k-fold-cross-validation",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "Review: \\(k\\)-Fold Cross Validation",
    "text": "Review: \\(k\\)-Fold Cross Validation\nWe can use k-fold cross-validation to estimate the typical error in our model predictions for new data:\n\nDivide the data into \\(k\\) folds (or groups) of approximately equal size.\n\nRepeat the following procedures for each fold \\(j = 1,2,...,k\\):\n\nRemove fold \\(j\\) from the data set.\n\nFit a model using the data in the other \\(k-1\\) folds (training).\n\nUse this model to predict the responses for the \\(n_j\\) cases in fold \\(j\\): \\(\\hat{y}_1, ..., \\hat{y}_{n_j}\\).\n\nCalculate the MAE for fold \\(j\\) (testing): \\(\\text{MAE}_j = \\frac{1}{n_j}\\sum_{i=1}^{n_j} |y_i - \\hat{y}_i|\\).\n\nCombine this information into one measure of model quality: \\[\\text{CV}_{(k)} = \\frac{1}{k} \\sum_{j=1}^k \\text{MAE}_j\\]\n\n\n\n\n\n\n\n\nVocabulary\n\n\n\n\n\n\nA tuning parameter is parameter or quantity upon which an algorithm depends whose value is selected or tuned to “optimize” the algorithm.\n\nFor \\(k\\)-fold CV, the tuning parameter is \\(k\\), where \\(2 \\le k \\le n\\), where \\(n\\) is the number of observations."
  },
  {
    "objectID": "Intro-CV.html#set-up",
    "href": "Intro-CV.html#set-up",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "Set Up",
    "text": "Set Up\n\nLoad Libraries\n\nlibrary(tidyverse)\n\nError in library(tidyverse): there is no package called 'tidyverse'\n\nlibrary(tidymodels)\n\nError in library(tidymodels): there is no package called 'tidymodels'\n\nlibrary(readxl)\n\nError in library(readxl): there is no package called 'readxl'\n\n# Load data\ndata(trees)\n\n\n\nRename Columns (Variables)\n\nRename Girth to diameter\nRename Height to height\n\n\n# Rename columns\ntrees = trees %&gt;% \n  rename(diameter = Girth, height = Height) %&gt;%\n  # Only have height and diameter be the columns \n  select(height, diameter)\n\nError in trees %&gt;% rename(diameter = Girth, height = Height) %&gt;% select(height, : could not find function \"%&gt;%\"\n\n\n\n\nVisualization (Scatter Plot)\n\n# Create a scatter plot\nggplot(trees, aes(x = diameter, y = height)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") \n\nError in ggplot(trees, aes(x = diameter, y = height)): could not find function \"ggplot\"\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\nHeight appears to be ___ with diameter\n\n\n\n\n\n# Step 1: Model specification\nlm_spec &lt;- linear_reg() %&gt;% \n  # Output Y is quantitative\n  set_mode(\"regression\") %&gt;%\n  # Want regression to be lienar\n  set_engine(\"lm\")\n\nError in linear_reg() %&gt;% set_mode(\"regression\") %&gt;% set_engine(\"lm\"): could not find function \"%&gt;%\"\n\n\n\n# Step 2: Model estimation\ntree_model &lt;- lm_spec %&gt;% fit( height ~ diameter, data = trees)\n\nError in lm_spec %&gt;% fit(height ~ diameter, data = trees): could not find function \"%&gt;%\""
  },
  {
    "objectID": "Intro-CV.html#fold-cross-validation",
    "href": "Intro-CV.html#fold-cross-validation",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "10-Fold Cross Validation",
    "text": "10-Fold Cross Validation\n\nProcedure\n\nRandomly split the data into 10 folds\nBuilt model 10 times, leaving 1 test fold out each time\nEvaluate each model on the test fold (using MAE/MSE and R-squared as error metrics)\n\n\n# For reproducibility\nset.seed(244)\n\ntree_model_cv = lm_spec %&gt;% \n# fit_resamples() function is for fitting on folds\nfit_resamples(\n  # Specify the relationship\n  height ~ diameter, \n  # vfold_cv makes CV folds randomly from\n  # trees data set\n  resamples = vfold_cv(trees, v = 10), \n  # Specify the error metrics\n  # (MAE, square root MSE, R^2)\n  metrics = metric_set(mae, rmse, rsq)\n  )\n\nError in lm_spec %&gt;% fit_resamples(height ~ diameter, resamples = vfold_cv(trees, : could not find function \"%&gt;%\"\n\n\n\ntree_model_cv %&gt;% collect_metrics()\n\nError in tree_model_cv %&gt;% collect_metrics(): could not find function \"%&gt;%\"\n\n\n\n\nSummarizing\n\n# Get fold-by-fold results\n# Get info for each test fold \ntree_model_cv %&gt;% unnest(.metrics) %&gt;% \nfilter(.metric == \"mae\")\n\nError in tree_model_cv %&gt;% unnest(.metrics) %&gt;% filter(.metric == \"mae\"): could not find function \"%&gt;%\"\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\nBased on my random folds above, the prediction error (MAE) was best for fold ___ and worst for fold ___"
  },
  {
    "objectID": "Intro-CV.html#exercise-1-in-sample-metrics",
    "href": "Intro-CV.html#exercise-1-in-sample-metrics",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 1: In-Sample Metrics",
    "text": "EXERCISE 1: In-Sample Metrics\nUse the health_data data to build two separate models of height:\n\n# STEP 2: model estimation\nmodel_1 &lt;- lm_spec %&gt;% \n  fit(height ~ hip + weight + thigh + knee + ankle, data = health_data)\n\nError in lm_spec %&gt;% fit(height ~ hip + weight + thigh + knee + ankle, : could not find function \"%&gt;%\"\n\nmodel_2 &lt;- lm_spec %&gt;% \n  fit(height ~ chest * age * weight  + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, data = health_data)\n\nError in lm_spec %&gt;% fit(height ~ chest * age * weight + abdomen + hip + : could not find function \"%&gt;%\"\n\n\nCalculate the in-sample R-squared for both models:\n\n# IN-SAMPLE R^2 for model_1 = ???\nmodel_1 %&gt;% glance()\n\nError in model_1 %&gt;% glance(): could not find function \"%&gt;%\"\n\n# IN-SAMPLE R^2 for model_2 = ???\nmodel_2 %&gt;% glance()\n\nError in model_2 %&gt;% glance(): could not find function \"%&gt;%\"\n\n\n\nANSWER. The R2 value for the first model is about 0.366, and for the second model, it’s 0.526.\n\nCalculate the in-sample MAE for both models:\n\n# IN-SAMPLE MAE for model_1 = ???\nmodel_1 %&gt;% \n  augment(new_data = health_data) %&gt;% \n  mae(truth = height, estimate = .pred)\n\nError in model_1 %&gt;% augment(new_data = health_data) %&gt;% mae(truth = height, : could not find function \"%&gt;%\"\n\n# IN-SAMPLE MAE for model_2 = ???\nmodel_2 %&gt;% \n  augment(new_data = health_data) %&gt;% \n  mae(truth = height, estimate = .pred)\n\nError in model_2 %&gt;% augment(new_data = health_data) %&gt;% mae(truth = height, : could not find function \"%&gt;%\"\n\n\n\nANSWER. Based on the in-sample MAE(i.e. the MAE of the same data used to build/train the model), it appears that model 2 (whose MAE is about 3.666) is better than model 1 (whose MAE is about 3.48)"
  },
  {
    "objectID": "Intro-CV.html#exercise-2-in-sample-model-comparison",
    "href": "Intro-CV.html#exercise-2-in-sample-model-comparison",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 2: In-Sample Model Comparison",
    "text": "EXERCISE 2: In-Sample Model Comparison\nWhich model seems “better” by the in-sample metrics you calculated above? Any concerns about either of these models?\n\nAnswer above! The concern is that we are using the same data that we built the modelw ith to evaluate the model’s error/performace, which means that the “better looking” model might be overfit (might be overly specific to the data used to build it)"
  },
  {
    "objectID": "Intro-CV.html#exercise-3-10-fold-cv",
    "href": "Intro-CV.html#exercise-3-10-fold-cv",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 3: 10-Fold CV",
    "text": "EXERCISE 3: 10-Fold CV\nComplete the code to run 10-fold cross-validation for our two models.\nmodel_1: height ~ hip + weight + thigh + knee + ankle\nmodel_2: height ~ chest * age * weight  + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist\n\n# 10-fold cross-validation for model_1\nset.seed(244)\nmodel_1_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    height ~ hip + weight + thigh + knee + ankle,\n    resamples= vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae, rsq)\n  )\n\nError in lm_spec %&gt;% fit_resamples(height ~ hip + weight + thigh + knee + : could not find function \"%&gt;%\"\n\n\n\n# 10-fold cross-validation for model_2\nset.seed(253)\nmodel_2_cv &lt;-lm_spec %&gt;% \n  fit_resamples(\n    height ~ chest * age * weight  + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n    resamples= vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae, rsq)\n  )\n\nError in lm_spec %&gt;% fit_resamples(height ~ chest * age * weight + abdomen + : could not find function \"%&gt;%\""
  },
  {
    "objectID": "Intro-CV.html#exercise-4-calculating-the-cv-mae",
    "href": "Intro-CV.html#exercise-4-calculating-the-cv-mae",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 4: Calculating the CV MAE",
    "text": "EXERCISE 4: Calculating the CV MAE\n\nUse collect_metrics() to obtain the cross-validated MAE and \\(R^2\\) for both models.\n\n\nmodel_1_cv %&gt;% \n  collect_metrics()\n\nError in model_1_cv %&gt;% collect_metrics(): could not find function \"%&gt;%\"\n\nmodel_2_cv %&gt;% \n  collect_metrics()\n\nError in model_2_cv %&gt;% collect_metrics(): could not find function \"%&gt;%\"\n\n\n\nInterpret the cross-validated MAE and \\(R^2\\) for model_1.\n\n\nANSWER. We expect our first model to produce predictions of height that are roughly off by 4.13 (the observed MAE) on average. For the first model, we expect i to explain roughly 0.28 (28%) of the variablity (based on the R2 value) in the observed heights of patients in the data set."
  },
  {
    "objectID": "Intro-CV.html#exercise-5-fold-by-fold-results",
    "href": "Intro-CV.html#exercise-5-fold-by-fold-results",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 5: Fold-By-Fold Results",
    "text": "EXERCISE 5: Fold-By-Fold Results\nThe command collect_metrics() gave the final CV MAE, or the average MAE across all 10 test folds. The command unnest(.metrics) provides the MAE from each test fold.\n\nObtain the fold-by-fold results for the model_1 cross-validation procedure using unnest(.metrics).\n\n\nmodel_1_cv %&gt;% \n  unnest(.metrics) %&gt;%  \n  filter (.metric == \"mae\")\n\nError in model_1_cv %&gt;% unnest(.metrics) %&gt;% filter(.metric == \"mae\"): could not find function \"%&gt;%\"\n\n\n\nWhich fold had the worst average prediction error and what was it?\n\n\nYOUR ANSWER HERE. For me, fold 5 had the worse (highest) MAE (which was 10.9)\n\n\nRecall that collect_metrics() reported a final CV MAE of ___ for model_1. Confirm this calculation by wrangling the fold-by-fold results from part a.\n\n\nmodel_1_cv %&gt;% \n  unnest(.metrics) %&gt;%  \n  filter (.metric == \"mae\")%&gt;%  \n  summarize(mean(.estimate))\n\nError in model_1_cv %&gt;% unnest(.metrics) %&gt;% filter(.metric == \"mae\") %&gt;% : could not find function \"%&gt;%\""
  },
  {
    "objectID": "Intro-CV.html#exercise-6-comparing-models",
    "href": "Intro-CV.html#exercise-6-comparing-models",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 6: Comparing Models",
    "text": "EXERCISE 6: Comparing Models\nFill in the table below to summarize the in-sample and 10-fold CV MAE for both models.\n\n\n\nModel\nIN-SAMPLE MAE\n10-fold CV MAE\n\n\n\n\nmodel_1\n3.28\n4.13\n\n\nmodel_2\n3.37\n6.28\n\n\n\n\nBased on the in-sample MAE alone, which model appears better?\n\n\nYOUR ANSWER HERE\n\n\nBased on the CV MAE alone, which model appears better?\n\n\nYOUR ANSWER HERE\n\n\nBased on all of these results, which model would you pick?\n\n\nYOUR ANSWER HERE\n\n\nDo the in-sample and CV MAE suggest that model_1 is overfit to our health_data sample data? What about model_2?\n\n\nIt looks like the MAE is roughly similar for when it’s measued in sample (3.48) versus when it is tested on “new” data (each test fold held out). However, model 2 seems overfit because its predictions for new patient data (giving on MAE of 6.28) are much worse than its predictions for patients in our data smple (MAE of 3.37)"
  },
  {
    "objectID": "Intro-CV.html#exercise-7-loocv",
    "href": "Intro-CV.html#exercise-7-loocv",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 7: LOOCV",
    "text": "EXERCISE 7: LOOCV\n\nReconsider model_1. Instead of estimating its prediction accuracy using the 10-fold CV MAE, use the LOOCV MAE. THINK: How many people are in our health_data sample?\n\n\nmodel_1_loocv = lm_spec %&gt;%\n  fit_resamples(\n    height ~ hip + weight + thigh + knee + ankle,\n    resamples = vfold_cv(health_data, v = nrow(health_data)),\n    metrics = metric_set(mae)\n  )\n\nError in lm_spec %&gt;% fit_resamples(height ~ hip + weight + thigh + knee + : could not find function \"%&gt;%\"\n\n\n\nHow does the LOOCV MAE compare to the 10-fold CV MAE of ___? NOTE: These are just two different approaches to estimating the same thing: the typical prediction error when applying our model to new data. Thus we should expect them to be similar.\n\n\nANSWER.\n\n\nExplain why we technically don’t need to set.seed() for the LOOCV algorithm.\n\n\nANSWER."
  }
]